{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:40.913547500Z",
     "start_time": "2023-09-13T11:49:40.897918400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sampo.scheduler.selection.neural_net import NeuralNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_dataset = pd.read_csv('dataset_mod.csv', index_col='index')\n",
    "for col in train_dataset.columns[:-1]:\n",
    "    train_dataset[col] = train_dataset[col].apply(lambda x: float(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:41.297376300Z",
     "start_time": "2023-09-13T11:49:41.226997300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "input_parameters = 13\n",
    "layer_size = 15\n",
    "layer_count = 6\n",
    "classification_size = 2\n",
    "learning_rate = 0.007\n",
    "\n",
    "model = NeuralNet(input_parameters, layer_size, layer_count, classification_size, learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:41.597875200Z",
     "start_time": "2023-09-13T11:49:41.566585200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:42.900032300Z",
     "start_time": "2023-09-13T11:49:42.868765300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0    -0.431248  0.163774 -0.439644  0.078385 -0.100235 -0.100235  0.071216   \n1    -0.142899  0.535020 -0.768132  0.375618 -0.236551 -0.236551 -0.172504   \n2    -2.139163  0.668828  2.280655  0.712180  2.523848  2.523848  2.386550   \n3    -0.076357 -0.140131 -0.762575  0.050138 -0.264224 -0.264224 -0.221980   \n4    -0.142899  0.535020 -0.768132  0.375618 -0.236551 -0.236551 -0.172504   \n...        ...       ...       ...       ...       ...       ...       ...   \n1995  0.189811 -1.035411 -0.610118 -0.982248 -0.363466 -0.363466 -0.399415   \n1996  1.298847 -1.360059 -0.373946 -1.228777 -0.645499 -0.645499 -0.903662   \n1997  1.520654 -1.368336 -0.162431 -1.464193 -0.685397 -0.685397 -0.974994   \n1998  0.677787 -0.059682  1.108385 -0.099326 -0.508367 -0.508367 -0.658483   \n1999  1.520654 -1.368336 -0.162431 -1.464193 -0.685397 -0.685397 -0.974994   \n\n             7         8         9        10        11        12  label  \n0     0.184498  0.406744 -0.876387  0.045492  0.741346 -0.142857      0  \n1    -0.101004 -0.226645  0.575085 -0.126582  0.457406 -0.142857      0  \n2     2.300270 -2.081261 -2.031834  0.002473 -2.098049 -0.142857      0  \n3    -0.457958  0.748588 -0.164149 -0.161514  0.399764 -0.142857      0  \n4    -0.101004 -0.226645  0.575085 -0.126582  0.457406 -0.142857      0  \n...        ...       ...       ...       ...       ...       ...    ...  \n1995 -0.257116  0.815440 -0.555008 -0.286788  0.193049 -0.142857      1  \n1996 -1.039082  0.111741  2.102949 -0.298655 -0.394412 -0.142857      1  \n1997 -0.808293  0.190465  1.592419 -0.365806 -0.477516 -0.142857      1  \n1998 -0.860796  1.591849 -0.441814 -0.067850 -0.108772 -0.142857      1  \n1999 -0.808293  0.190465  1.592419 -0.365806 -0.477516 -0.142857      1  \n\n[2000 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.431248</td>\n      <td>0.163774</td>\n      <td>-0.439644</td>\n      <td>0.078385</td>\n      <td>-0.100235</td>\n      <td>-0.100235</td>\n      <td>0.071216</td>\n      <td>0.184498</td>\n      <td>0.406744</td>\n      <td>-0.876387</td>\n      <td>0.045492</td>\n      <td>0.741346</td>\n      <td>-0.142857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.142899</td>\n      <td>0.535020</td>\n      <td>-0.768132</td>\n      <td>0.375618</td>\n      <td>-0.236551</td>\n      <td>-0.236551</td>\n      <td>-0.172504</td>\n      <td>-0.101004</td>\n      <td>-0.226645</td>\n      <td>0.575085</td>\n      <td>-0.126582</td>\n      <td>0.457406</td>\n      <td>-0.142857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-2.139163</td>\n      <td>0.668828</td>\n      <td>2.280655</td>\n      <td>0.712180</td>\n      <td>2.523848</td>\n      <td>2.523848</td>\n      <td>2.386550</td>\n      <td>2.300270</td>\n      <td>-2.081261</td>\n      <td>-2.031834</td>\n      <td>0.002473</td>\n      <td>-2.098049</td>\n      <td>-0.142857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.076357</td>\n      <td>-0.140131</td>\n      <td>-0.762575</td>\n      <td>0.050138</td>\n      <td>-0.264224</td>\n      <td>-0.264224</td>\n      <td>-0.221980</td>\n      <td>-0.457958</td>\n      <td>0.748588</td>\n      <td>-0.164149</td>\n      <td>-0.161514</td>\n      <td>0.399764</td>\n      <td>-0.142857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.142899</td>\n      <td>0.535020</td>\n      <td>-0.768132</td>\n      <td>0.375618</td>\n      <td>-0.236551</td>\n      <td>-0.236551</td>\n      <td>-0.172504</td>\n      <td>-0.101004</td>\n      <td>-0.226645</td>\n      <td>0.575085</td>\n      <td>-0.126582</td>\n      <td>0.457406</td>\n      <td>-0.142857</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>0.189811</td>\n      <td>-1.035411</td>\n      <td>-0.610118</td>\n      <td>-0.982248</td>\n      <td>-0.363466</td>\n      <td>-0.363466</td>\n      <td>-0.399415</td>\n      <td>-0.257116</td>\n      <td>0.815440</td>\n      <td>-0.555008</td>\n      <td>-0.286788</td>\n      <td>0.193049</td>\n      <td>-0.142857</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>1.298847</td>\n      <td>-1.360059</td>\n      <td>-0.373946</td>\n      <td>-1.228777</td>\n      <td>-0.645499</td>\n      <td>-0.645499</td>\n      <td>-0.903662</td>\n      <td>-1.039082</td>\n      <td>0.111741</td>\n      <td>2.102949</td>\n      <td>-0.298655</td>\n      <td>-0.394412</td>\n      <td>-0.142857</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>1.520654</td>\n      <td>-1.368336</td>\n      <td>-0.162431</td>\n      <td>-1.464193</td>\n      <td>-0.685397</td>\n      <td>-0.685397</td>\n      <td>-0.974994</td>\n      <td>-0.808293</td>\n      <td>0.190465</td>\n      <td>1.592419</td>\n      <td>-0.365806</td>\n      <td>-0.477516</td>\n      <td>-0.142857</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>0.677787</td>\n      <td>-0.059682</td>\n      <td>1.108385</td>\n      <td>-0.099326</td>\n      <td>-0.508367</td>\n      <td>-0.508367</td>\n      <td>-0.658483</td>\n      <td>-0.860796</td>\n      <td>1.591849</td>\n      <td>-0.441814</td>\n      <td>-0.067850</td>\n      <td>-0.108772</td>\n      <td>-0.142857</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>1.520654</td>\n      <td>-1.368336</td>\n      <td>-0.162431</td>\n      <td>-1.464193</td>\n      <td>-0.685397</td>\n      <td>-0.685397</td>\n      <td>-0.974994</td>\n      <td>-0.808293</td>\n      <td>0.190465</td>\n      <td>1.592419</td>\n      <td>-0.365806</td>\n      <td>-0.477516</td>\n      <td>-0.142857</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_dataset.drop(columns=['label']))\n",
    "scaled_dataset = scaler.transform(train_dataset.drop(columns=['label']))\n",
    "scaled_dataset = pd.DataFrame(scaled_dataset, columns=train_dataset.drop(columns=['label']).columns)\n",
    "train_dataset = pd.concat([scaled_dataset, train_dataset['label']], axis=1)\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:43.571156Z",
     "start_time": "2023-09-13T11:49:43.485943500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_dataset.drop(columns=['label']), train_dataset['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:44.670977700Z",
     "start_time": "2023-09-13T11:49:44.655352200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/1350], Loss: 1.0366\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.6149\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.6662\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.3705\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.9379\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.4882\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.6418\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.7126\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.6691\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.6666\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.3901\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.6673\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.4271\n",
      "Epoch [2/2], Step [100/1350], Loss: 1.0516\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.6156\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.6548\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.3636\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.9493\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.4827\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.6309\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.7033\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.6737\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.6714\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.3827\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.6722\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.4201\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5178\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.6161\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.3854\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.5827\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.6361\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.5079\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.6263\n",
      "Epoch [1/2], Step [800/1350], Loss: 1.1810\n",
      "Epoch [1/2], Step [900/1350], Loss: 1.0753\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.6364\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.3500\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.3637\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.3497\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5105\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.6184\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.3787\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.5688\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.6385\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.5043\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.6130\n",
      "Epoch [2/2], Step [800/1350], Loss: 1.1923\n",
      "Epoch [2/2], Step [900/1350], Loss: 1.0877\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.6418\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.3458\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.3590\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.3459\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.6887\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.5097\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.6266\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.6058\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.9799\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.3438\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.4051\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.6821\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.4606\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.6632\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.3550\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.6501\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.5540\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.6918\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.5080\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.6189\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.5947\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.9895\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.3405\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.4003\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.6749\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.4572\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.6578\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.3509\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.6534\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.5432\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5419\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.3694\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.4394\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.4401\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.5214\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.6514\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.4086\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.5350\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.5161\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.6330\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.4768\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.4972\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.5321\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5305\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.3656\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.4334\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.4339\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.5156\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.6445\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.4053\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.5255\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.5117\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.6307\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.4704\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.4941\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.5226\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.3551\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.5210\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.6042\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.3322\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.5850\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.6153\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.6310\n",
      "Epoch [1/2], Step [800/1350], Loss: 1.2345\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.3162\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.6428\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.3964\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.3955\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.5655\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.3519\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.5126\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.5991\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.3304\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.5765\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.6125\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.6276\n",
      "Epoch [2/2], Step [800/1350], Loss: 1.2399\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.3158\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.6397\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.3924\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.3915\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.5588\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5566\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.7236\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.5932\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.3888\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.6449\n",
      "Epoch [1/2], Step [600/1350], Loss: 1.2466\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.4825\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.4535\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.4376\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.6203\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.7076\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.6473\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.5516\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5505\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.7313\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.5884\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.3848\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.6370\n",
      "Epoch [2/2], Step [600/1350], Loss: 1.2517\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.4791\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.4476\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.4344\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.6157\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.7040\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.6469\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.5449\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5944\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.3509\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.6322\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.3820\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.7014\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.3496\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.8455\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.3152\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.3743\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.4422\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.4816\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.3801\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.3734\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5917\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.3486\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.6287\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.3789\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.6997\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.3473\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.8539\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.3150\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.3716\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.4378\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.4743\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.3773\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.3708\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5887\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.4361\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.5728\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.5358\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.4940\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.3697\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.3312\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.6086\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.6314\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.4328\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.6017\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.3139\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.3136\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5841\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.4318\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.5679\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.5299\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.4903\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.3674\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.3299\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.6028\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.6250\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.4310\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.5960\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.3138\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.3136\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5814\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.5232\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.5229\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.3248\n",
      "Epoch [1/2], Step [500/1350], Loss: 1.0731\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.4270\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.4981\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.3133\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.4282\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.3145\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.5905\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.3410\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.4688\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5811\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.5166\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.5165\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.3239\n",
      "Epoch [2/2], Step [500/1350], Loss: 1.0808\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.4238\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.4917\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.3133\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.4251\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.3143\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.5834\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.3397\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.4628\n",
      "Epoch [1/2], Step [100/1350], Loss: 0.5057\n",
      "Epoch [1/2], Step [200/1350], Loss: 0.6390\n",
      "Epoch [1/2], Step [300/1350], Loss: 0.3133\n",
      "Epoch [1/2], Step [400/1350], Loss: 0.3652\n",
      "Epoch [1/2], Step [500/1350], Loss: 0.3384\n",
      "Epoch [1/2], Step [600/1350], Loss: 0.3133\n",
      "Epoch [1/2], Step [700/1350], Loss: 0.4587\n",
      "Epoch [1/2], Step [800/1350], Loss: 0.5023\n",
      "Epoch [1/2], Step [900/1350], Loss: 0.6101\n",
      "Epoch [1/2], Step [1000/1350], Loss: 0.5533\n",
      "Epoch [1/2], Step [1100/1350], Loss: 0.7309\n",
      "Epoch [1/2], Step [1200/1350], Loss: 0.3733\n",
      "Epoch [1/2], Step [1300/1350], Loss: 0.3823\n",
      "Epoch [2/2], Step [100/1350], Loss: 0.5029\n",
      "Epoch [2/2], Step [200/1350], Loss: 0.6456\n",
      "Epoch [2/2], Step [300/1350], Loss: 0.3133\n",
      "Epoch [2/2], Step [400/1350], Loss: 0.3634\n",
      "Epoch [2/2], Step [500/1350], Loss: 0.3372\n",
      "Epoch [2/2], Step [600/1350], Loss: 0.3133\n",
      "Epoch [2/2], Step [700/1350], Loss: 0.4554\n",
      "Epoch [2/2], Step [800/1350], Loss: 0.4990\n",
      "Epoch [2/2], Step [900/1350], Loss: 0.6063\n",
      "Epoch [2/2], Step [1000/1350], Loss: 0.5510\n",
      "Epoch [2/2], Step [1100/1350], Loss: 0.7378\n",
      "Epoch [2/2], Step [1200/1350], Loss: 0.3710\n",
      "Epoch [2/2], Step [1300/1350], Loss: 0.3808\n"
     ]
    }
   ],
   "source": [
    "from sampo.scheduler.selection.validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "s = cross_val_score(train_dataset=pd.concat([x_train, y_train], axis=1),\n",
    "                    target_column='label',\n",
    "                    scorer=accuracy_score,\n",
    "                    model=model,\n",
    "                    epochs=2,\n",
    "                    folds=10,\n",
    "                    shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(230.9019, grad_fn=<SelectBackward0>)\n",
      "1 tensor(342.5922, grad_fn=<SelectBackward0>)\n",
      "2 tensor(341.3734, grad_fn=<SelectBackward0>)\n",
      "3 tensor(476.5591, grad_fn=<SelectBackward0>)\n",
      "4 tensor(98.2785, grad_fn=<SelectBackward0>)\n",
      "5 tensor(98.6533, grad_fn=<SelectBackward0>)\n",
      "6 tensor(241.6574, grad_fn=<SelectBackward0>)\n",
      "7 tensor(328.5511, grad_fn=<SelectBackward0>)\n",
      "8 tensor(402.8818, grad_fn=<SelectBackward0>)\n",
      "9 tensor(145.0562, grad_fn=<SelectBackward0>)\n",
      "10 tensor(305.0154, grad_fn=<SelectBackward0>)\n",
      "11 tensor(170.0357, grad_fn=<SelectBackward0>)\n",
      "12 tensor(119.5020, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "index = [-10000] * input_parameters\n",
    "for fold in list(model.parameters())[0]:\n",
    "    for i in range(len(fold)):\n",
    "        index[i] = max(index[i], fold[i])\n",
    "for i in range(len(index)):\n",
    "    print(i, index[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:01:13.992197300Z",
     "start_time": "2023-09-13T09:01:13.960690600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86, 0.8533333333333334, 0.88, 0.9333333333333333, 0.96, 0.9533333333333334, 0.9733333333333334, 0.9466666666666667, 0.9533333333333334, 0.9733333333333334]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:01:17.855217Z",
     "start_time": "2023-09-13T09:01:17.839622700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test_dataset = x_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:01:38.532117500Z",
     "start_time": "2023-09-13T09:01:38.516531900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0.968"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict([torch.Tensor(v) for v in test_dataset.iloc[:, :].values])\n",
    "array = []\n",
    "label_test = y_test.to_numpy()\n",
    "for i in range(len(predicted)):\n",
    "    flag = 0 if predicted[i][0] > predicted[i][1] else 1\n",
    "    array.append(int(flag == label_test[i]))\n",
    "sum(array) / len(array)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:02:02.040715Z",
     "start_time": "2023-09-13T09:02:01.929708300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:47:36.643993700Z",
     "start_time": "2023-09-13T09:47:36.634747900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:47:36.853829500Z",
     "start_time": "2023-09-13T09:47:36.831506600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:47:37.023969700Z",
     "start_time": "2023-09-13T09:47:37.003797300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:47:37.209203800Z",
     "start_time": "2023-09-13T09:47:37.153748200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sampo.scheduler.selection.neural_net import NeuralNet\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import torch\n",
    "# import numpy as np\n",
    "#\n",
    "# data = pd.read_csv('dataset_mod.csv', index_col='index')\n",
    "# gs_params = {\n",
    "#     'layer_size': [7, 15],\n",
    "#     'layer_count': [3, 6],\n",
    "#     'learning_rate': [0.1, 0.01]\n",
    "# }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:46:44.546527200Z",
     "start_time": "2023-09-13T11:46:44.530873700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "\n",
    "# from sampo.scheduler.selection.metrics import one_hot_encode\n",
    "#\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data.drop(columns=['label']), data['label'])\n",
    "# x_train = [torch.Tensor(v) for v in x_train.values]\n",
    "# x_test = [torch.Tensor(v) for v in x_test.values]\n",
    "# y_train = [torch.Tensor(one_hot_encode(v, 2)) for v in y_train.values]\n",
    "# y_test = [torch.Tensor(one_hot_encode(v, 2)) for v in y_test.values]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:48:48.499280100Z",
     "start_time": "2023-09-13T11:48:48.429381400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n tensor([0., 1.]),\n tensor([1., 0.]),\n tensor([1., 0.]),\n ...]"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:48:49.658245Z",
     "start_time": "2023-09-13T11:48:49.362549600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "(1500, 13)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(x_train), len(x_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:48:51.597638500Z",
     "start_time": "2023-09-13T11:48:51.574828Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# def accuracy(_estimator, _x_test, _y_test):\n",
    "#     return accuracy_score(_y_test, _estimator.predict(_x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:48:52.947087800Z",
     "start_time": "2023-09-13T11:48:52.928421800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1200], Loss: 0.7384\n",
      "Epoch [1/10], Step [200/1200], Loss: 0.6882\n",
      "Epoch [1/10], Step [300/1200], Loss: 0.6806\n",
      "Epoch [1/10], Step [400/1200], Loss: 0.7202\n",
      "Epoch [1/10], Step [500/1200], Loss: 0.7130\n",
      "Epoch [1/10], Step [600/1200], Loss: 0.7051\n",
      "Epoch [1/10], Step [700/1200], Loss: 0.6933\n",
      "Epoch [1/10], Step [800/1200], Loss: 0.7136\n",
      "Epoch [1/10], Step [900/1200], Loss: 0.7167\n",
      "Epoch [1/10], Step [1000/1200], Loss: 0.7140\n",
      "Epoch [1/10], Step [1100/1200], Loss: 0.7230\n",
      "Epoch [1/10], Step [1200/1200], Loss: 0.7087\n",
      "Epoch [2/10], Step [100/1200], Loss: 0.6608\n",
      "Epoch [2/10], Step [200/1200], Loss: 0.7047\n",
      "Epoch [2/10], Step [300/1200], Loss: 0.6766\n",
      "Epoch [2/10], Step [400/1200], Loss: 0.7130\n",
      "Epoch [2/10], Step [500/1200], Loss: 0.7119\n",
      "Epoch [2/10], Step [600/1200], Loss: 0.6518\n",
      "Epoch [2/10], Step [700/1200], Loss: 0.6440\n",
      "Epoch [2/10], Step [800/1200], Loss: 0.7132\n",
      "Epoch [2/10], Step [900/1200], Loss: 0.7262\n",
      "Epoch [2/10], Step [1000/1200], Loss: 0.7133\n",
      "Epoch [2/10], Step [1100/1200], Loss: 0.7191\n",
      "Epoch [2/10], Step [1200/1200], Loss: 0.7077\n",
      "Epoch [3/10], Step [100/1200], Loss: 0.6319\n",
      "Epoch [3/10], Step [200/1200], Loss: 0.7202\n",
      "Epoch [3/10], Step [300/1200], Loss: 0.6735\n",
      "Epoch [3/10], Step [400/1200], Loss: 0.6942\n",
      "Epoch [3/10], Step [500/1200], Loss: 0.7007\n",
      "Epoch [3/10], Step [600/1200], Loss: 0.6338\n",
      "Epoch [3/10], Step [700/1200], Loss: 0.6242\n",
      "Epoch [3/10], Step [800/1200], Loss: 0.7093\n",
      "Epoch [3/10], Step [900/1200], Loss: 0.7258\n",
      "Epoch [3/10], Step [1000/1200], Loss: 0.7030\n",
      "Epoch [3/10], Step [1100/1200], Loss: 0.7046\n",
      "Epoch [3/10], Step [1200/1200], Loss: 0.7000\n",
      "Epoch [4/10], Step [100/1200], Loss: 0.6148\n",
      "Epoch [4/10], Step [200/1200], Loss: 0.7306\n",
      "Epoch [4/10], Step [300/1200], Loss: 0.6789\n",
      "Epoch [4/10], Step [400/1200], Loss: 0.6834\n",
      "Epoch [4/10], Step [500/1200], Loss: 0.6965\n",
      "Epoch [4/10], Step [600/1200], Loss: 0.6178\n",
      "Epoch [4/10], Step [700/1200], Loss: 0.6079\n",
      "Epoch [4/10], Step [800/1200], Loss: 0.7015\n",
      "Epoch [4/10], Step [900/1200], Loss: 0.7273\n",
      "Epoch [4/10], Step [1000/1200], Loss: 0.6904\n",
      "Epoch [4/10], Step [1100/1200], Loss: 0.6955\n",
      "Epoch [4/10], Step [1200/1200], Loss: 0.6926\n",
      "Epoch [5/10], Step [100/1200], Loss: 0.6004\n",
      "Epoch [5/10], Step [200/1200], Loss: 0.7432\n",
      "Epoch [5/10], Step [300/1200], Loss: 0.6871\n",
      "Epoch [5/10], Step [400/1200], Loss: 0.6693\n",
      "Epoch [5/10], Step [500/1200], Loss: 0.6936\n",
      "Epoch [5/10], Step [600/1200], Loss: 0.6050\n",
      "Epoch [5/10], Step [700/1200], Loss: 0.5904\n",
      "Epoch [5/10], Step [800/1200], Loss: 0.6899\n",
      "Epoch [5/10], Step [900/1200], Loss: 0.7229\n",
      "Epoch [5/10], Step [1000/1200], Loss: 0.6779\n",
      "Epoch [5/10], Step [1100/1200], Loss: 0.6879\n",
      "Epoch [5/10], Step [1200/1200], Loss: 0.6872\n",
      "Epoch [6/10], Step [100/1200], Loss: 0.5877\n",
      "Epoch [6/10], Step [200/1200], Loss: 0.7496\n",
      "Epoch [6/10], Step [300/1200], Loss: 0.6869\n",
      "Epoch [6/10], Step [400/1200], Loss: 0.6630\n",
      "Epoch [6/10], Step [500/1200], Loss: 0.6930\n",
      "Epoch [6/10], Step [600/1200], Loss: 0.5944\n",
      "Epoch [6/10], Step [700/1200], Loss: 0.5793\n",
      "Epoch [6/10], Step [800/1200], Loss: 0.6868\n",
      "Epoch [6/10], Step [900/1200], Loss: 0.7247\n",
      "Epoch [6/10], Step [1000/1200], Loss: 0.6700\n",
      "Epoch [6/10], Step [1100/1200], Loss: 0.6798\n",
      "Epoch [6/10], Step [1200/1200], Loss: 0.6786\n",
      "Epoch [7/10], Step [100/1200], Loss: 0.5760\n",
      "Epoch [7/10], Step [200/1200], Loss: 0.7567\n",
      "Epoch [7/10], Step [300/1200], Loss: 0.6889\n",
      "Epoch [7/10], Step [400/1200], Loss: 0.6560\n",
      "Epoch [7/10], Step [500/1200], Loss: 0.6926\n",
      "Epoch [7/10], Step [600/1200], Loss: 0.5844\n",
      "Epoch [7/10], Step [700/1200], Loss: 0.5680\n",
      "Epoch [7/10], Step [800/1200], Loss: 0.6830\n",
      "Epoch [7/10], Step [900/1200], Loss: 0.7262\n",
      "Epoch [7/10], Step [1000/1200], Loss: 0.6632\n",
      "Epoch [7/10], Step [1100/1200], Loss: 0.6744\n",
      "Epoch [7/10], Step [1200/1200], Loss: 0.6761\n",
      "Epoch [8/10], Step [100/1200], Loss: 0.5658\n",
      "Epoch [8/10], Step [200/1200], Loss: 0.7634\n",
      "Epoch [8/10], Step [300/1200], Loss: 0.6899\n",
      "Epoch [8/10], Step [400/1200], Loss: 0.6480\n",
      "Epoch [8/10], Step [500/1200], Loss: 0.6898\n",
      "Epoch [8/10], Step [600/1200], Loss: 0.5753\n",
      "Epoch [8/10], Step [700/1200], Loss: 0.5577\n",
      "Epoch [8/10], Step [800/1200], Loss: 0.6812\n",
      "Epoch [8/10], Step [900/1200], Loss: 0.7311\n",
      "Epoch [8/10], Step [1000/1200], Loss: 0.6521\n",
      "Epoch [8/10], Step [1100/1200], Loss: 0.6676\n",
      "Epoch [8/10], Step [1200/1200], Loss: 0.6718\n",
      "Epoch [9/10], Step [100/1200], Loss: 0.5557\n",
      "Epoch [9/10], Step [200/1200], Loss: 0.7694\n",
      "Epoch [9/10], Step [300/1200], Loss: 0.6945\n",
      "Epoch [9/10], Step [400/1200], Loss: 0.6429\n",
      "Epoch [9/10], Step [500/1200], Loss: 0.6895\n",
      "Epoch [9/10], Step [600/1200], Loss: 0.5668\n",
      "Epoch [9/10], Step [700/1200], Loss: 0.5485\n",
      "Epoch [9/10], Step [800/1200], Loss: 0.6813\n",
      "Epoch [9/10], Step [900/1200], Loss: 0.7285\n",
      "Epoch [9/10], Step [1000/1200], Loss: 0.6489\n",
      "Epoch [9/10], Step [1100/1200], Loss: 0.6622\n",
      "Epoch [9/10], Step [1200/1200], Loss: 0.6667\n",
      "Epoch [10/10], Step [100/1200], Loss: 0.5460\n",
      "Epoch [10/10], Step [200/1200], Loss: 0.7780\n",
      "Epoch [10/10], Step [300/1200], Loss: 0.6929\n",
      "Epoch [10/10], Step [400/1200], Loss: 0.6367\n",
      "Epoch [10/10], Step [500/1200], Loss: 0.6863\n",
      "Epoch [10/10], Step [600/1200], Loss: 0.5580\n",
      "Epoch [10/10], Step [700/1200], Loss: 0.5392\n",
      "Epoch [10/10], Step [800/1200], Loss: 0.6772\n",
      "Epoch [10/10], Step [900/1200], Loss: 0.7281\n",
      "Epoch [10/10], Step [1000/1200], Loss: 0.6430\n",
      "Epoch [10/10], Step [1100/1200], Loss: 0.6570\n",
      "Epoch [10/10], Step [1200/1200], Loss: 0.6619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\Ð˜Ð²Ð°Ð½\\AppData\\Local\\Temp\\ipykernel_34164\\2015115388.py\", line 2, in accuracy\n",
      "    return accuracy_score(_y_test, _estimator.predict(_x_test))\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 220, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 104, in _check_targets\n",
      "    raise ValueError(\"{0} is not supported\".format(y_type))\n",
      "ValueError: unknown is not supported\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1200], Loss: 0.6181\n",
      "Epoch [1/10], Step [200/1200], Loss: 0.7478\n",
      "Epoch [1/10], Step [300/1200], Loss: 0.6278\n",
      "Epoch [1/10], Step [400/1200], Loss: 0.6535\n",
      "Epoch [1/10], Step [500/1200], Loss: 0.6778\n",
      "Epoch [1/10], Step [600/1200], Loss: 0.5879\n",
      "Epoch [1/10], Step [700/1200], Loss: 0.5871\n",
      "Epoch [1/10], Step [800/1200], Loss: 0.6882\n",
      "Epoch [1/10], Step [900/1200], Loss: 0.7009\n",
      "Epoch [1/10], Step [1000/1200], Loss: 0.6856\n",
      "Epoch [1/10], Step [1100/1200], Loss: 0.6979\n",
      "Epoch [1/10], Step [1200/1200], Loss: 0.6972\n",
      "Epoch [2/10], Step [100/1200], Loss: 0.6733\n",
      "Epoch [2/10], Step [200/1200], Loss: 0.6535\n",
      "Epoch [2/10], Step [300/1200], Loss: 0.6669\n",
      "Epoch [2/10], Step [400/1200], Loss: 0.6857\n",
      "Epoch [2/10], Step [500/1200], Loss: 0.7294\n",
      "Epoch [2/10], Step [600/1200], Loss: 0.5844\n",
      "Epoch [2/10], Step [700/1200], Loss: 0.5802\n",
      "Epoch [2/10], Step [800/1200], Loss: 0.7223\n",
      "Epoch [2/10], Step [900/1200], Loss: 0.7407\n",
      "Epoch [2/10], Step [1000/1200], Loss: 0.7154\n",
      "Epoch [2/10], Step [1100/1200], Loss: 0.7266\n",
      "Epoch [2/10], Step [1200/1200], Loss: 0.7232\n",
      "Epoch [3/10], Step [100/1200], Loss: 0.6881\n",
      "Epoch [3/10], Step [200/1200], Loss: 0.6139\n",
      "Epoch [3/10], Step [300/1200], Loss: 0.6884\n",
      "Epoch [3/10], Step [400/1200], Loss: 0.6964\n",
      "Epoch [3/10], Step [500/1200], Loss: 0.7496\n",
      "Epoch [3/10], Step [600/1200], Loss: 0.5810\n",
      "Epoch [3/10], Step [700/1200], Loss: 0.5788\n",
      "Epoch [3/10], Step [800/1200], Loss: 0.7432\n",
      "Epoch [3/10], Step [900/1200], Loss: 0.7679\n",
      "Epoch [3/10], Step [1000/1200], Loss: 0.7356\n",
      "Epoch [3/10], Step [1100/1200], Loss: 0.7476\n",
      "Epoch [3/10], Step [1200/1200], Loss: 0.7451\n",
      "Epoch [4/10], Step [100/1200], Loss: 0.7013\n",
      "Epoch [4/10], Step [200/1200], Loss: 0.5884\n",
      "Epoch [4/10], Step [300/1200], Loss: 0.7058\n",
      "Epoch [4/10], Step [400/1200], Loss: 0.7027\n",
      "Epoch [4/10], Step [500/1200], Loss: 0.7652\n",
      "Epoch [4/10], Step [600/1200], Loss: 0.5786\n",
      "Epoch [4/10], Step [700/1200], Loss: 0.5768\n",
      "Epoch [4/10], Step [800/1200], Loss: 0.7446\n",
      "Epoch [4/10], Step [900/1200], Loss: 0.7791\n",
      "Epoch [4/10], Step [1000/1200], Loss: 0.7194\n",
      "Epoch [4/10], Step [1100/1200], Loss: 0.7486\n",
      "Epoch [4/10], Step [1200/1200], Loss: 0.7540\n",
      "Epoch [5/10], Step [100/1200], Loss: 0.6995\n",
      "Epoch [5/10], Step [200/1200], Loss: 0.5752\n",
      "Epoch [5/10], Step [300/1200], Loss: 0.7095\n",
      "Epoch [5/10], Step [400/1200], Loss: 0.7053\n",
      "Epoch [5/10], Step [500/1200], Loss: 0.7720\n",
      "Epoch [5/10], Step [600/1200], Loss: 0.5775\n",
      "Epoch [5/10], Step [700/1200], Loss: 0.5752\n",
      "Epoch [5/10], Step [800/1200], Loss: 0.7475\n",
      "Epoch [5/10], Step [900/1200], Loss: 0.7933\n",
      "Epoch [5/10], Step [1000/1200], Loss: 0.7192\n",
      "Epoch [5/10], Step [1100/1200], Loss: 0.7489\n",
      "Epoch [5/10], Step [1200/1200], Loss: 0.7551\n",
      "Epoch [6/10], Step [100/1200], Loss: 0.6939\n",
      "Epoch [6/10], Step [200/1200], Loss: 0.5670\n",
      "Epoch [6/10], Step [300/1200], Loss: 0.7075\n",
      "Epoch [6/10], Step [400/1200], Loss: 0.6990\n",
      "Epoch [6/10], Step [500/1200], Loss: 0.7836\n",
      "Epoch [6/10], Step [600/1200], Loss: 0.5754\n",
      "Epoch [6/10], Step [700/1200], Loss: 0.5713\n",
      "Epoch [6/10], Step [800/1200], Loss: 0.7479\n",
      "Epoch [6/10], Step [900/1200], Loss: 0.8032\n",
      "Epoch [6/10], Step [1000/1200], Loss: 0.7188\n",
      "Epoch [6/10], Step [1100/1200], Loss: 0.7560\n",
      "Epoch [6/10], Step [1200/1200], Loss: 0.7613\n",
      "Epoch [7/10], Step [100/1200], Loss: 0.6944\n",
      "Epoch [7/10], Step [200/1200], Loss: 0.5506\n",
      "Epoch [7/10], Step [300/1200], Loss: 0.7121\n",
      "Epoch [7/10], Step [400/1200], Loss: 0.6942\n",
      "Epoch [7/10], Step [500/1200], Loss: 0.7841\n",
      "Epoch [7/10], Step [600/1200], Loss: 0.5728\n",
      "Epoch [7/10], Step [700/1200], Loss: 0.5693\n",
      "Epoch [7/10], Step [800/1200], Loss: 0.7442\n",
      "Epoch [7/10], Step [900/1200], Loss: 0.8074\n",
      "Epoch [7/10], Step [1000/1200], Loss: 0.7139\n",
      "Epoch [7/10], Step [1100/1200], Loss: 0.7562\n",
      "Epoch [7/10], Step [1200/1200], Loss: 0.7610\n",
      "Epoch [8/10], Step [100/1200], Loss: 0.6879\n",
      "Epoch [8/10], Step [200/1200], Loss: 0.5429\n",
      "Epoch [8/10], Step [300/1200], Loss: 0.7135\n",
      "Epoch [8/10], Step [400/1200], Loss: 0.6919\n",
      "Epoch [8/10], Step [500/1200], Loss: 0.7875\n",
      "Epoch [8/10], Step [600/1200], Loss: 0.5712\n",
      "Epoch [8/10], Step [700/1200], Loss: 0.5680\n",
      "Epoch [8/10], Step [800/1200], Loss: 0.7310\n",
      "Epoch [8/10], Step [900/1200], Loss: 0.8104\n",
      "Epoch [8/10], Step [1000/1200], Loss: 0.7072\n",
      "Epoch [8/10], Step [1100/1200], Loss: 0.7524\n",
      "Epoch [8/10], Step [1200/1200], Loss: 0.7590\n",
      "Epoch [9/10], Step [100/1200], Loss: 0.6791\n",
      "Epoch [9/10], Step [200/1200], Loss: 0.5359\n",
      "Epoch [9/10], Step [300/1200], Loss: 0.7120\n",
      "Epoch [9/10], Step [400/1200], Loss: 0.6863\n",
      "Epoch [9/10], Step [500/1200], Loss: 0.7880\n",
      "Epoch [9/10], Step [600/1200], Loss: 0.5694\n",
      "Epoch [9/10], Step [700/1200], Loss: 0.5669\n",
      "Epoch [9/10], Step [800/1200], Loss: 0.7240\n",
      "Epoch [9/10], Step [900/1200], Loss: 0.8123\n",
      "Epoch [9/10], Step [1000/1200], Loss: 0.7010\n",
      "Epoch [9/10], Step [1100/1200], Loss: 0.7456\n",
      "Epoch [9/10], Step [1200/1200], Loss: 0.7438\n",
      "Epoch [10/10], Step [100/1200], Loss: 0.6581\n",
      "Epoch [10/10], Step [200/1200], Loss: 0.5317\n",
      "Epoch [10/10], Step [300/1200], Loss: 0.7132\n",
      "Epoch [10/10], Step [400/1200], Loss: 0.6818\n",
      "Epoch [10/10], Step [500/1200], Loss: 0.7894\n",
      "Epoch [10/10], Step [600/1200], Loss: 0.5681\n",
      "Epoch [10/10], Step [700/1200], Loss: 0.5663\n",
      "Epoch [10/10], Step [800/1200], Loss: 0.7257\n",
      "Epoch [10/10], Step [900/1200], Loss: 0.8134\n",
      "Epoch [10/10], Step [1000/1200], Loss: 0.6908\n",
      "Epoch [10/10], Step [1100/1200], Loss: 0.7364\n",
      "Epoch [10/10], Step [1200/1200], Loss: 0.7405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\Ð˜Ð²Ð°Ð½\\AppData\\Local\\Temp\\ipykernel_34164\\2015115388.py\", line 2, in accuracy\n",
      "    return accuracy_score(_y_test, _estimator.predict(_x_test))\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 220, in accuracy_score\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\SAMPO\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 104, in _check_targets\n",
      "    raise ValueError(\"{0} is not supported\".format(y_type))\n",
      "ValueError: unknown is not supported\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1200], Loss: 0.7356\n",
      "Epoch [1/10], Step [200/1200], Loss: 0.6224\n",
      "Epoch [1/10], Step [300/1200], Loss: 0.7404\n",
      "Epoch [1/10], Step [400/1200], Loss: 0.7037\n",
      "Epoch [1/10], Step [500/1200], Loss: 0.6446\n",
      "Epoch [1/10], Step [600/1200], Loss: 0.6418\n",
      "Epoch [1/10], Step [700/1200], Loss: 0.7034\n",
      "Epoch [1/10], Step [800/1200], Loss: 0.7683\n",
      "Epoch [1/10], Step [900/1200], Loss: 0.7817\n",
      "Epoch [1/10], Step [1000/1200], Loss: 0.7702\n",
      "Epoch [1/10], Step [1100/1200], Loss: 0.7862\n",
      "Epoch [1/10], Step [1200/1200], Loss: 0.7862\n",
      "Epoch [2/10], Step [100/1200], Loss: 0.7689\n",
      "Epoch [2/10], Step [200/1200], Loss: 0.5793\n",
      "Epoch [2/10], Step [300/1200], Loss: 0.7769\n",
      "Epoch [2/10], Step [400/1200], Loss: 0.7044\n",
      "Epoch [2/10], Step [500/1200], Loss: 0.6204\n",
      "Epoch [2/10], Step [600/1200], Loss: 0.6186\n",
      "Epoch [2/10], Step [700/1200], Loss: 0.7036\n",
      "Epoch [2/10], Step [800/1200], Loss: 0.7930\n",
      "Epoch [2/10], Step [900/1200], Loss: 0.8083\n",
      "Epoch [2/10], Step [1000/1200], Loss: 0.7893\n",
      "Epoch [2/10], Step [1100/1200], Loss: 0.8043\n",
      "Epoch [2/10], Step [1200/1200], Loss: 0.8034\n",
      "Epoch [3/10], Step [100/1200], Loss: 0.7813\n",
      "Epoch [3/10], Step [200/1200], Loss: 0.5593\n",
      "Epoch [3/10], Step [300/1200], Loss: 0.7949\n",
      "Epoch [3/10], Step [400/1200], Loss: 0.7033\n",
      "Epoch [3/10], Step [500/1200], Loss: 0.6043\n",
      "Epoch [3/10], Step [600/1200], Loss: 0.6038\n",
      "Epoch [3/10], Step [700/1200], Loss: 0.7022\n",
      "Epoch [3/10], Step [800/1200], Loss: 0.8082\n",
      "Epoch [3/10], Step [900/1200], Loss: 0.8244\n",
      "Epoch [3/10], Step [1000/1200], Loss: 0.7997\n",
      "Epoch [3/10], Step [1100/1200], Loss: 0.8140\n",
      "Epoch [3/10], Step [1200/1200], Loss: 0.8133\n",
      "Epoch [4/10], Step [100/1200], Loss: 0.7877\n",
      "Epoch [4/10], Step [200/1200], Loss: 0.5469\n",
      "Epoch [4/10], Step [300/1200], Loss: 0.8054\n",
      "Epoch [4/10], Step [400/1200], Loss: 0.7018\n",
      "Epoch [4/10], Step [500/1200], Loss: 0.5973\n",
      "Epoch [4/10], Step [600/1200], Loss: 0.5970\n",
      "Epoch [4/10], Step [700/1200], Loss: 0.6998\n",
      "Epoch [4/10], Step [800/1200], Loss: 0.8147\n",
      "Epoch [4/10], Step [900/1200], Loss: 0.8323\n",
      "Epoch [4/10], Step [1000/1200], Loss: 0.8036\n",
      "Epoch [4/10], Step [1100/1200], Loss: 0.8181\n",
      "Epoch [4/10], Step [1200/1200], Loss: 0.8180\n",
      "Epoch [5/10], Step [100/1200], Loss: 0.7895\n",
      "Epoch [5/10], Step [200/1200], Loss: 0.5396\n",
      "Epoch [5/10], Step [300/1200], Loss: 0.8101\n",
      "Epoch [5/10], Step [400/1200], Loss: 0.6925\n",
      "Epoch [5/10], Step [500/1200], Loss: 0.5985\n",
      "Epoch [5/10], Step [600/1200], Loss: 0.5992\n",
      "Epoch [5/10], Step [700/1200], Loss: 0.6801\n",
      "Epoch [5/10], Step [800/1200], Loss: 0.8127\n",
      "Epoch [5/10], Step [900/1200], Loss: 0.8327\n",
      "Epoch [5/10], Step [1000/1200], Loss: 0.7973\n",
      "Epoch [5/10], Step [1100/1200], Loss: 0.8102\n",
      "Epoch [5/10], Step [1200/1200], Loss: 0.8086\n",
      "Epoch [6/10], Step [100/1200], Loss: 0.7772\n",
      "Epoch [6/10], Step [200/1200], Loss: 0.5410\n",
      "Epoch [6/10], Step [300/1200], Loss: 0.8051\n",
      "Epoch [6/10], Step [400/1200], Loss: 0.6791\n",
      "Epoch [6/10], Step [500/1200], Loss: 0.5982\n",
      "Epoch [6/10], Step [600/1200], Loss: 0.5933\n",
      "Epoch [6/10], Step [700/1200], Loss: 0.6781\n",
      "Epoch [6/10], Step [800/1200], Loss: 0.8193\n",
      "Epoch [6/10], Step [900/1200], Loss: 0.8422\n",
      "Epoch [6/10], Step [1000/1200], Loss: 0.8033\n",
      "Epoch [6/10], Step [1100/1200], Loss: 0.8169\n",
      "Epoch [6/10], Step [1200/1200], Loss: 0.8168\n",
      "Epoch [7/10], Step [100/1200], Loss: 0.7824\n",
      "Epoch [7/10], Step [200/1200], Loss: 0.5319\n",
      "Epoch [7/10], Step [300/1200], Loss: 0.8136\n",
      "Epoch [7/10], Step [400/1200], Loss: 0.6762\n",
      "Epoch [7/10], Step [500/1200], Loss: 0.5954\n",
      "Epoch [7/10], Step [600/1200], Loss: 0.5891\n",
      "Epoch [7/10], Step [700/1200], Loss: 0.6755\n",
      "Epoch [7/10], Step [800/1200], Loss: 0.8228\n",
      "Epoch [7/10], Step [900/1200], Loss: 0.8471\n",
      "Epoch [7/10], Step [1000/1200], Loss: 0.8034\n",
      "Epoch [7/10], Step [1100/1200], Loss: 0.8184\n",
      "Epoch [7/10], Step [1200/1200], Loss: 0.8194\n",
      "Epoch [8/10], Step [100/1200], Loss: 0.7825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimator = NeuralNet()\n",
    "# gs = GridSearchCV(estimator, gs_params, cv=5, scoring=accuracy)\n",
    "# gs.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T11:49:30.918701300Z",
     "start_time": "2023-09-13T11:49:00.413279600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "sklearn.model_selection._search.GridSearchCV"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T10:55:58.052666200Z",
     "start_time": "2023-09-13T10:55:58.036680200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
