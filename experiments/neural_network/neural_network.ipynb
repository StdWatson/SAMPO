{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_parameters = 14\n",
    "layer_size = 5\n",
    "layer_count = 3\n",
    "classification_size = 3\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:26:40.243406300Z",
     "start_time": "2023-08-30T14:26:40.233370800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-1.5927e-01, -1.9320e-01,  9.5098e-02, -2.0807e-01,  1.7157e-01,\n          -1.6885e-02, -6.7384e-02, -2.5003e-01, -2.0265e-01, -6.5255e-03,\n          -1.0617e-01, -1.9848e-01,  6.7585e-02],\n         [-4.9080e-02,  1.8151e-01,  1.3348e-01,  1.4507e-01,  2.3773e-01,\n           8.8335e-02,  3.3662e-02,  4.5203e-02, -1.3676e-01, -4.6430e-02,\n          -2.6078e-01,  1.8373e-01,  1.0592e-01],\n         [-1.5682e-01,  3.6403e-02,  6.2144e-02,  9.2655e-05,  3.5083e-02,\n          -2.3186e-01, -1.0585e-01, -2.0260e-01,  2.7534e-01,  2.5723e-01,\n          -2.0047e-01,  2.3451e-02, -2.0298e-01],\n         [ 8.9845e-02,  6.6567e-02, -2.3717e-01,  1.9971e-01,  3.2292e-02,\n          -1.0897e-01,  1.3988e-01,  2.1391e-01, -2.3284e-01,  9.0283e-02,\n           1.8073e-01,  2.4850e-01,  1.6076e-01],\n         [ 7.0289e-02,  1.5040e-01,  3.4290e-02,  2.4460e-01, -2.1923e-02,\n           1.1609e-01, -1.0063e-01, -3.9139e-02, -8.3488e-02, -1.6468e-01,\n          -1.5083e-01,  2.4637e-01, -1.2561e-01]], requires_grad=True),\n Parameter containing:\n tensor([ 0.1160,  0.1417, -0.2389,  0.1887,  0.0196], requires_grad=True)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sampo.scheduler.selection.neural_net import NeuralNet, load_dataset\n",
    "\n",
    "model = NeuralNet(input_parameters, layer_size, layer_count, classification_size)\n",
    "list(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:26:40.423526700Z",
     "start_time": "2023-08-30T14:26:40.393105900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-1.5927e-01, -1.9320e-01,  9.5098e-02, -2.0807e-01,  1.7157e-01,\n          -1.6885e-02, -6.7384e-02, -2.5003e-01, -2.0265e-01, -6.5255e-03,\n          -1.0617e-01, -1.9848e-01,  6.7585e-02],\n         [-4.9080e-02,  1.8151e-01,  1.3348e-01,  1.4507e-01,  2.3773e-01,\n           8.8335e-02,  3.3662e-02,  4.5203e-02, -1.3676e-01, -4.6430e-02,\n          -2.6078e-01,  1.8373e-01,  1.0592e-01],\n         [-1.5682e-01,  3.6403e-02,  6.2144e-02,  9.2655e-05,  3.5083e-02,\n          -2.3186e-01, -1.0585e-01, -2.0260e-01,  2.7534e-01,  2.5723e-01,\n          -2.0047e-01,  2.3451e-02, -2.0298e-01],\n         [ 8.9845e-02,  6.6567e-02, -2.3717e-01,  1.9971e-01,  3.2292e-02,\n          -1.0897e-01,  1.3988e-01,  2.1391e-01, -2.3284e-01,  9.0283e-02,\n           1.8073e-01,  2.4850e-01,  1.6076e-01],\n         [ 7.0289e-02,  1.5040e-01,  3.4290e-02,  2.4460e-01, -2.1923e-02,\n           1.1609e-01, -1.0063e-01, -3.9139e-02, -8.3488e-02, -1.6468e-01,\n          -1.5083e-01,  2.4637e-01, -1.2561e-01]], requires_grad=True),\n Parameter containing:\n tensor([ 0.1160,  0.1417, -0.2389,  0.1887,  0.0196], requires_grad=True)]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(10):\n",
    "#     print(f'Loop {i}/10')\n",
    "x_train, x_test, y_train, y_test = load_dataset('dataset.csv')\n",
    "model.fit(x_train, y_train, 100)\n",
    "list(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:26:41.513226200Z",
     "start_time": "2023-08-30T14:26:41.055213300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([0., 0., 1.]), tensor([0., 1., 0.]), tensor([1., 0., 0.])]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:26:42.398321100Z",
     "start_time": "2023-08-30T14:26:42.383354900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.score(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:27:06.443447700Z",
     "start_time": "2023-08-30T14:27:06.413133400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sampo.scheduler.selection.neural_net import NeuralNet\n",
    "\n",
    "train_dataset = pd.read_csv('dataset.csv', index_col='index')\n",
    "for col in train_dataset.columns[:-1]:\n",
    "    train_dataset[col] = train_dataset[col].apply(lambda x: float(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:27:25.722995700Z",
     "start_time": "2023-08-30T14:27:25.673218200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1     2          3     4    5    6    7      8      9  \\\nindex                                                                        \n0      127.0  0.859823  46.0  29.814961  46.0  1.0  1.0  3.0   93.0  424.0   \n2      190.0  0.906255  68.0  30.084211  68.0  1.0  1.0  4.0  138.0  527.0   \n1      112.0  1.121457  34.0  35.151786  34.0  1.0  1.0  3.0   69.0  412.0   \n3      182.0  0.977690  66.0  33.574176  66.0  1.0  1.0  3.0   66.0  356.0   \n5      156.0  0.719637  58.0  28.128205  58.0  1.0  1.0  3.0   94.0  400.0   \n4      127.0  1.049286  41.0  32.751969  41.0  1.0  1.0  3.0   71.0  482.0   \n6      191.0  0.644718  66.0  27.039267  66.0  1.0  1.0  3.0   90.0  646.0   \n7      133.0  0.757486  38.0  29.436090  38.0  1.0  1.0  3.0   77.0  560.0   \n8      195.0  0.614343  78.0  27.212821  78.0  1.0  1.0  3.0   78.0  408.0   \n\n           10      11     12  label  \nindex                                \n0       729.0   881.0  258.0      0  \n2       892.0   976.0  292.0      0  \n1       667.0   883.0  226.0      0  \n3      1260.0  1344.0  900.0      1  \n5       800.0   860.0  356.0      1  \n4       749.0  1009.0  242.0      1  \n6      1248.0  1568.0  580.0      2  \n7       865.0  1193.0  274.0      2  \n8      1228.0  1304.0  804.0      2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>127.0</td>\n      <td>0.859823</td>\n      <td>46.0</td>\n      <td>29.814961</td>\n      <td>46.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>93.0</td>\n      <td>424.0</td>\n      <td>729.0</td>\n      <td>881.0</td>\n      <td>258.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>190.0</td>\n      <td>0.906255</td>\n      <td>68.0</td>\n      <td>30.084211</td>\n      <td>68.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>138.0</td>\n      <td>527.0</td>\n      <td>892.0</td>\n      <td>976.0</td>\n      <td>292.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>112.0</td>\n      <td>1.121457</td>\n      <td>34.0</td>\n      <td>35.151786</td>\n      <td>34.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>69.0</td>\n      <td>412.0</td>\n      <td>667.0</td>\n      <td>883.0</td>\n      <td>226.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>182.0</td>\n      <td>0.977690</td>\n      <td>66.0</td>\n      <td>33.574176</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>66.0</td>\n      <td>356.0</td>\n      <td>1260.0</td>\n      <td>1344.0</td>\n      <td>900.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>156.0</td>\n      <td>0.719637</td>\n      <td>58.0</td>\n      <td>28.128205</td>\n      <td>58.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>94.0</td>\n      <td>400.0</td>\n      <td>800.0</td>\n      <td>860.0</td>\n      <td>356.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>127.0</td>\n      <td>1.049286</td>\n      <td>41.0</td>\n      <td>32.751969</td>\n      <td>41.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>71.0</td>\n      <td>482.0</td>\n      <td>749.0</td>\n      <td>1009.0</td>\n      <td>242.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>191.0</td>\n      <td>0.644718</td>\n      <td>66.0</td>\n      <td>27.039267</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>90.0</td>\n      <td>646.0</td>\n      <td>1248.0</td>\n      <td>1568.0</td>\n      <td>580.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>133.0</td>\n      <td>0.757486</td>\n      <td>38.0</td>\n      <td>29.436090</td>\n      <td>38.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>77.0</td>\n      <td>560.0</td>\n      <td>865.0</td>\n      <td>1193.0</td>\n      <td>274.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>195.0</td>\n      <td>0.614343</td>\n      <td>78.0</td>\n      <td>27.212821</td>\n      <td>78.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>78.0</td>\n      <td>408.0</td>\n      <td>1228.0</td>\n      <td>1304.0</td>\n      <td>804.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_parameters = 14\n",
    "layer_size = 5\n",
    "layer_count = 3\n",
    "classification_size = 3\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = NeuralNet(input_parameters, layer_size, layer_count, classification_size, learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:27:26.193024800Z",
     "start_time": "2023-08-30T14:27:26.153299100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.5171, Accuracy: 0/3 (0.00%)\n",
      "\n",
      "Test set: Average loss: 0.5171, Accuracy: 2/3 (66.67%)\n",
      "\n",
      "Test set: Average loss: 0.5171, Accuracy: 0/3 (0.00%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0, 66.66666666666667, 0.0]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:27:27.964020500Z",
     "start_time": "2023-08-30T14:27:26.913334700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [2/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [3/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [4/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [5/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [6/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [7/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [8/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [9/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [10/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [11/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [12/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [13/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [14/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [15/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [16/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [17/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [18/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [19/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [20/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [21/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [22/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [23/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [24/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [25/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [26/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [27/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [28/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [29/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [30/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [31/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [32/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [33/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [34/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [35/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [36/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [37/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [38/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [39/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [40/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [41/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [42/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [43/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [44/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [45/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [46/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [47/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [48/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [49/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [50/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [51/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [52/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [53/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [54/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [55/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [56/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [57/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [58/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [59/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [60/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [61/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [62/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [63/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [64/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [65/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [66/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [67/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [68/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [69/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [70/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [71/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [72/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [73/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [74/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [75/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [76/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [77/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [78/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [79/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [80/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [81/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [82/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [83/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [84/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [85/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [86/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [87/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [88/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [89/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [90/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [91/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [92/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [93/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [94/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [95/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [96/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [97/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [98/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [99/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [100/100], Step [100/135], Loss: 0.5514\n",
      "Test set: Average loss: 0.0368, Accuracy: 5/15 (33.33%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [2/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [3/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [4/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [5/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [6/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [7/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [8/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [9/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [10/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [11/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [12/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [13/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [14/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [15/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [16/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [17/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [18/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [19/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [20/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [21/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [22/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [23/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [24/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [25/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [26/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [27/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [28/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [29/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [30/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [31/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [32/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [33/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [34/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [35/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [36/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [37/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [38/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [39/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [40/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [41/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [42/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [43/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [44/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [45/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [46/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [47/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [48/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [49/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [50/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [51/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [52/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [53/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [54/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [55/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [56/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [57/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [58/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [59/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [60/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [61/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [62/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [63/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [64/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [65/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [66/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [67/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [68/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [69/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [70/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [71/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [72/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [73/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [74/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [75/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [76/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [77/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [78/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [79/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [80/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [81/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [82/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [83/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [84/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [85/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [86/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [87/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [88/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [89/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [90/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [91/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [92/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [93/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [94/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [95/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [96/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [97/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [98/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [99/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [100/100], Step [100/135], Loss: 1.5514\n",
      "Test set: Average loss: 0.0368, Accuracy: 6/15 (40.00%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 0.5520\n",
      "Epoch [2/100], Step [100/135], Loss: 0.5520\n",
      "Epoch [3/100], Step [100/135], Loss: 0.5521\n",
      "Epoch [4/100], Step [100/135], Loss: 0.5521\n",
      "Epoch [5/100], Step [100/135], Loss: 0.5522\n",
      "Epoch [6/100], Step [100/135], Loss: 0.5522\n",
      "Epoch [7/100], Step [100/135], Loss: 0.5523\n",
      "Epoch [8/100], Step [100/135], Loss: 0.5523\n",
      "Epoch [9/100], Step [100/135], Loss: 0.5524\n",
      "Epoch [10/100], Step [100/135], Loss: 0.5524\n",
      "Epoch [11/100], Step [100/135], Loss: 0.5525\n",
      "Epoch [12/100], Step [100/135], Loss: 0.5526\n",
      "Epoch [13/100], Step [100/135], Loss: 0.5527\n",
      "Epoch [14/100], Step [100/135], Loss: 0.5527\n",
      "Epoch [15/100], Step [100/135], Loss: 0.5528\n",
      "Epoch [16/100], Step [100/135], Loss: 0.5529\n",
      "Epoch [17/100], Step [100/135], Loss: 0.5530\n",
      "Epoch [18/100], Step [100/135], Loss: 0.5531\n",
      "Epoch [19/100], Step [100/135], Loss: 0.5533\n",
      "Epoch [20/100], Step [100/135], Loss: 0.5534\n",
      "Epoch [21/100], Step [100/135], Loss: 0.5536\n",
      "Epoch [22/100], Step [100/135], Loss: 0.5537\n",
      "Epoch [23/100], Step [100/135], Loss: 0.5539\n",
      "Epoch [24/100], Step [100/135], Loss: 0.5541\n",
      "Epoch [25/100], Step [100/135], Loss: 0.5544\n",
      "Epoch [26/100], Step [100/135], Loss: 0.5546\n",
      "Epoch [27/100], Step [100/135], Loss: 0.5549\n",
      "Epoch [28/100], Step [100/135], Loss: 0.5553\n",
      "Epoch [29/100], Step [100/135], Loss: 0.5558\n",
      "Epoch [30/100], Step [100/135], Loss: 0.5563\n",
      "Epoch [31/100], Step [100/135], Loss: 0.5548\n",
      "Epoch [32/100], Step [100/135], Loss: 0.5553\n",
      "Epoch [33/100], Step [100/135], Loss: 0.5559\n",
      "Epoch [34/100], Step [100/135], Loss: 0.5567\n",
      "Epoch [35/100], Step [100/135], Loss: 0.5552\n",
      "Epoch [36/100], Step [100/135], Loss: 0.5559\n",
      "Epoch [37/100], Step [100/135], Loss: 0.5568\n",
      "Epoch [38/100], Step [100/135], Loss: 0.5553\n",
      "Epoch [39/100], Step [100/135], Loss: 0.5562\n",
      "Epoch [40/100], Step [100/135], Loss: 0.5550\n",
      "Epoch [41/100], Step [100/135], Loss: 0.5558\n",
      "Epoch [42/100], Step [100/135], Loss: 0.5568\n",
      "Epoch [43/100], Step [100/135], Loss: 0.5555\n",
      "Epoch [44/100], Step [100/135], Loss: 0.5564\n",
      "Epoch [45/100], Step [100/135], Loss: 0.5553\n",
      "Epoch [46/100], Step [100/135], Loss: 0.5562\n",
      "Epoch [47/100], Step [100/135], Loss: 0.5574\n",
      "Epoch [48/100], Step [100/135], Loss: 0.5560\n",
      "Epoch [49/100], Step [100/135], Loss: 0.5573\n",
      "Epoch [50/100], Step [100/135], Loss: 0.5560\n",
      "Epoch [51/100], Step [100/135], Loss: 0.5572\n",
      "Epoch [52/100], Step [100/135], Loss: 0.5560\n",
      "Epoch [53/100], Step [100/135], Loss: 0.5573\n",
      "Epoch [54/100], Step [100/135], Loss: 0.5561\n",
      "Epoch [55/100], Step [100/135], Loss: 0.5575\n",
      "Epoch [56/100], Step [100/135], Loss: 0.5563\n",
      "Epoch [57/100], Step [100/135], Loss: 0.5578\n",
      "Epoch [58/100], Step [100/135], Loss: 0.5565\n",
      "Epoch [59/100], Step [100/135], Loss: 0.5582\n",
      "Epoch [60/100], Step [100/135], Loss: 0.5569\n",
      "Epoch [61/100], Step [100/135], Loss: 0.5588\n",
      "Epoch [62/100], Step [100/135], Loss: 0.5568\n",
      "Epoch [63/100], Step [100/135], Loss: 0.5588\n",
      "Epoch [64/100], Step [100/135], Loss: 0.5575\n",
      "Epoch [65/100], Step [100/135], Loss: 0.5598\n",
      "Epoch [66/100], Step [100/135], Loss: 0.5579\n",
      "Epoch [67/100], Step [100/135], Loss: 0.5570\n",
      "Epoch [68/100], Step [100/135], Loss: 0.5592\n",
      "Epoch [69/100], Step [100/135], Loss: 0.5580\n",
      "Epoch [70/100], Step [100/135], Loss: 0.5607\n",
      "Epoch [71/100], Step [100/135], Loss: 0.5589\n",
      "Epoch [72/100], Step [100/135], Loss: 0.5579\n",
      "Epoch [73/100], Step [100/135], Loss: 0.5608\n",
      "Epoch [74/100], Step [100/135], Loss: 0.5591\n",
      "Epoch [75/100], Step [100/135], Loss: 0.5582\n",
      "Epoch [76/100], Step [100/135], Loss: 0.5613\n",
      "Epoch [77/100], Step [100/135], Loss: 0.5596\n",
      "Epoch [78/100], Step [100/135], Loss: 0.5588\n",
      "Epoch [79/100], Step [100/135], Loss: 0.5625\n",
      "Epoch [80/100], Step [100/135], Loss: 0.5608\n",
      "Epoch [81/100], Step [100/135], Loss: 0.5591\n",
      "Epoch [82/100], Step [100/135], Loss: 0.5632\n",
      "Epoch [83/100], Step [100/135], Loss: 0.5617\n",
      "Epoch [84/100], Step [100/135], Loss: 0.5604\n",
      "Epoch [85/100], Step [100/135], Loss: 0.5598\n",
      "Epoch [86/100], Step [100/135], Loss: 0.5594\n",
      "Epoch [87/100], Step [100/135], Loss: 0.5641\n",
      "Epoch [88/100], Step [100/135], Loss: 0.5627\n",
      "Epoch [89/100], Step [100/135], Loss: 0.5615\n",
      "Epoch [90/100], Step [100/135], Loss: 0.5612\n",
      "Epoch [91/100], Step [100/135], Loss: 0.5609\n",
      "Epoch [92/100], Step [100/135], Loss: 0.5606\n",
      "Epoch [93/100], Step [100/135], Loss: 0.5669\n",
      "Epoch [94/100], Step [100/135], Loss: 0.5614\n",
      "Epoch [95/100], Step [100/135], Loss: 0.5614\n",
      "Epoch [96/100], Step [100/135], Loss: 0.5687\n",
      "Epoch [97/100], Step [100/135], Loss: 0.5629\n",
      "Epoch [98/100], Step [100/135], Loss: 0.5632\n",
      "Epoch [99/100], Step [100/135], Loss: 0.5635\n",
      "Epoch [100/100], Step [100/135], Loss: 0.5640\n",
      "Test set: Average loss: 0.0368, Accuracy: 5/15 (33.33%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [2/100], Step [100/135], Loss: 0.5514\n",
      "Epoch [3/100], Step [100/135], Loss: 0.5515\n",
      "Epoch [4/100], Step [100/135], Loss: 0.5515\n",
      "Epoch [5/100], Step [100/135], Loss: 0.5557\n",
      "Epoch [6/100], Step [100/135], Loss: 0.5631\n",
      "Epoch [7/100], Step [100/135], Loss: 0.6210\n",
      "Epoch [8/100], Step [100/135], Loss: 0.7576\n",
      "Epoch [9/100], Step [100/135], Loss: 0.7836\n",
      "Epoch [10/100], Step [100/135], Loss: 0.7841\n",
      "Epoch [11/100], Step [100/135], Loss: 0.7844\n",
      "Epoch [12/100], Step [100/135], Loss: 0.7848\n",
      "Epoch [13/100], Step [100/135], Loss: 0.7851\n",
      "Epoch [14/100], Step [100/135], Loss: 0.7853\n",
      "Epoch [15/100], Step [100/135], Loss: 0.7856\n",
      "Epoch [16/100], Step [100/135], Loss: 0.7713\n",
      "Epoch [17/100], Step [100/135], Loss: 0.7715\n",
      "Epoch [18/100], Step [100/135], Loss: 0.7609\n",
      "Epoch [19/100], Step [100/135], Loss: 0.7611\n",
      "Epoch [20/100], Step [100/135], Loss: 0.7614\n",
      "Epoch [21/100], Step [100/135], Loss: 0.7617\n",
      "Epoch [22/100], Step [100/135], Loss: 0.7620\n",
      "Epoch [23/100], Step [100/135], Loss: 0.7622\n",
      "Epoch [24/100], Step [100/135], Loss: 0.7624\n",
      "Epoch [25/100], Step [100/135], Loss: 0.7626\n",
      "Epoch [26/100], Step [100/135], Loss: 0.7628\n",
      "Epoch [27/100], Step [100/135], Loss: 0.7630\n",
      "Epoch [28/100], Step [100/135], Loss: 0.7631\n",
      "Epoch [29/100], Step [100/135], Loss: 0.7632\n",
      "Epoch [30/100], Step [100/135], Loss: 0.7634\n",
      "Epoch [31/100], Step [100/135], Loss: 0.7985\n",
      "Epoch [32/100], Step [100/135], Loss: 0.7972\n",
      "Epoch [33/100], Step [100/135], Loss: 0.7970\n",
      "Epoch [34/100], Step [100/135], Loss: 0.8052\n",
      "Epoch [35/100], Step [100/135], Loss: 0.8056\n",
      "Epoch [36/100], Step [100/135], Loss: 0.8061\n",
      "Epoch [37/100], Step [100/135], Loss: 0.8066\n",
      "Epoch [38/100], Step [100/135], Loss: 0.8071\n",
      "Epoch [39/100], Step [100/135], Loss: 0.8076\n",
      "Epoch [40/100], Step [100/135], Loss: 0.8081\n",
      "Epoch [41/100], Step [100/135], Loss: 0.8086\n",
      "Epoch [42/100], Step [100/135], Loss: 0.8091\n",
      "Epoch [43/100], Step [100/135], Loss: 0.8095\n",
      "Epoch [44/100], Step [100/135], Loss: 0.7768\n",
      "Epoch [45/100], Step [100/135], Loss: 0.7771\n",
      "Epoch [46/100], Step [100/135], Loss: 0.7776\n",
      "Epoch [47/100], Step [100/135], Loss: 0.7780\n",
      "Epoch [48/100], Step [100/135], Loss: 0.7784\n",
      "Epoch [49/100], Step [100/135], Loss: 0.7787\n",
      "Epoch [50/100], Step [100/135], Loss: 0.7789\n",
      "Epoch [51/100], Step [100/135], Loss: 0.7790\n",
      "Epoch [52/100], Step [100/135], Loss: 0.7791\n",
      "Epoch [53/100], Step [100/135], Loss: 0.7792\n",
      "Epoch [54/100], Step [100/135], Loss: 0.7792\n",
      "Epoch [55/100], Step [100/135], Loss: 0.7792\n",
      "Epoch [56/100], Step [100/135], Loss: 0.7791\n",
      "Epoch [57/100], Step [100/135], Loss: 0.7790\n",
      "Epoch [58/100], Step [100/135], Loss: 0.7789\n",
      "Epoch [59/100], Step [100/135], Loss: 0.7788\n",
      "Epoch [60/100], Step [100/135], Loss: 0.7787\n",
      "Epoch [61/100], Step [100/135], Loss: 0.7785\n",
      "Epoch [62/100], Step [100/135], Loss: 0.7743\n",
      "Epoch [63/100], Step [100/135], Loss: 0.7741\n",
      "Epoch [64/100], Step [100/135], Loss: 0.7739\n",
      "Epoch [65/100], Step [100/135], Loss: 0.7755\n",
      "Epoch [66/100], Step [100/135], Loss: 0.7753\n",
      "Epoch [67/100], Step [100/135], Loss: 0.7750\n",
      "Epoch [68/100], Step [100/135], Loss: 0.7748\n",
      "Epoch [69/100], Step [100/135], Loss: 0.7838\n",
      "Epoch [70/100], Step [100/135], Loss: 0.7819\n",
      "Epoch [71/100], Step [100/135], Loss: 0.7819\n",
      "Epoch [72/100], Step [100/135], Loss: 0.7820\n",
      "Epoch [73/100], Step [100/135], Loss: 0.7821\n",
      "Epoch [74/100], Step [100/135], Loss: 0.7821\n",
      "Epoch [75/100], Step [100/135], Loss: 0.7822\n",
      "Epoch [76/100], Step [100/135], Loss: 0.7823\n",
      "Epoch [77/100], Step [100/135], Loss: 0.7582\n",
      "Epoch [78/100], Step [100/135], Loss: 0.7813\n",
      "Epoch [79/100], Step [100/135], Loss: 0.7577\n",
      "Epoch [80/100], Step [100/135], Loss: 0.7569\n",
      "Epoch [81/100], Step [100/135], Loss: 0.7567\n",
      "Epoch [82/100], Step [100/135], Loss: 0.7564\n",
      "Epoch [83/100], Step [100/135], Loss: 0.7561\n",
      "Epoch [84/100], Step [100/135], Loss: 0.7558\n",
      "Epoch [85/100], Step [100/135], Loss: 0.7555\n",
      "Epoch [86/100], Step [100/135], Loss: 0.7552\n",
      "Epoch [87/100], Step [100/135], Loss: 0.7697\n",
      "Epoch [88/100], Step [100/135], Loss: 0.7549\n",
      "Epoch [89/100], Step [100/135], Loss: 0.7540\n",
      "Epoch [90/100], Step [100/135], Loss: 0.7536\n",
      "Epoch [91/100], Step [100/135], Loss: 0.7533\n",
      "Epoch [92/100], Step [100/135], Loss: 0.7528\n",
      "Epoch [93/100], Step [100/135], Loss: 0.7524\n",
      "Epoch [94/100], Step [100/135], Loss: 0.7520\n",
      "Epoch [95/100], Step [100/135], Loss: 0.7582\n",
      "Epoch [96/100], Step [100/135], Loss: 0.7586\n",
      "Epoch [97/100], Step [100/135], Loss: 0.7588\n",
      "Epoch [98/100], Step [100/135], Loss: 0.7584\n",
      "Epoch [99/100], Step [100/135], Loss: 0.7583\n",
      "Epoch [100/100], Step [100/135], Loss: 0.7582\n",
      "Test set: Average loss: 0.1034, Accuracy: 6/15 (40.00%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 1.0468\n",
      "Epoch [2/100], Step [100/135], Loss: 1.3434\n",
      "Epoch [3/100], Step [100/135], Loss: 1.4149\n",
      "Epoch [4/100], Step [100/135], Loss: 1.4045\n",
      "Epoch [5/100], Step [100/135], Loss: 1.4035\n",
      "Epoch [6/100], Step [100/135], Loss: 1.4037\n",
      "Epoch [7/100], Step [100/135], Loss: 1.4040\n",
      "Epoch [8/100], Step [100/135], Loss: 1.4042\n",
      "Epoch [9/100], Step [100/135], Loss: 1.4044\n",
      "Epoch [10/100], Step [100/135], Loss: 1.4045\n",
      "Epoch [11/100], Step [100/135], Loss: 1.4046\n",
      "Epoch [12/100], Step [100/135], Loss: 1.4047\n",
      "Epoch [13/100], Step [100/135], Loss: 1.4048\n",
      "Epoch [14/100], Step [100/135], Loss: 1.4049\n",
      "Epoch [15/100], Step [100/135], Loss: 1.4050\n",
      "Epoch [16/100], Step [100/135], Loss: 1.4051\n",
      "Epoch [17/100], Step [100/135], Loss: 1.4160\n",
      "Epoch [18/100], Step [100/135], Loss: 1.4064\n",
      "Epoch [19/100], Step [100/135], Loss: 1.4243\n",
      "Epoch [20/100], Step [100/135], Loss: 1.4078\n",
      "Epoch [21/100], Step [100/135], Loss: 1.4245\n",
      "Epoch [22/100], Step [100/135], Loss: 1.4186\n",
      "Epoch [23/100], Step [100/135], Loss: 1.4258\n",
      "Epoch [24/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [25/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [26/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [27/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [28/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [29/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [30/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [31/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [32/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [33/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [34/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [35/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [36/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [37/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [38/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [39/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [40/100], Step [100/135], Loss: 1.4286\n",
      "Epoch [41/100], Step [100/135], Loss: 1.4271\n",
      "Epoch [42/100], Step [100/135], Loss: 1.4285\n",
      "Epoch [43/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [44/100], Step [100/135], Loss: 1.4284\n",
      "Epoch [45/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [46/100], Step [100/135], Loss: 1.4284\n",
      "Epoch [47/100], Step [100/135], Loss: 1.4268\n",
      "Epoch [48/100], Step [100/135], Loss: 1.4283\n",
      "Epoch [49/100], Step [100/135], Loss: 1.4267\n",
      "Epoch [50/100], Step [100/135], Loss: 1.4282\n",
      "Epoch [51/100], Step [100/135], Loss: 1.4267\n",
      "Epoch [52/100], Step [100/135], Loss: 1.4282\n",
      "Epoch [53/100], Step [100/135], Loss: 1.4266\n",
      "Epoch [54/100], Step [100/135], Loss: 1.4281\n",
      "Epoch [55/100], Step [100/135], Loss: 1.4282\n",
      "Epoch [56/100], Step [100/135], Loss: 1.4265\n",
      "Epoch [57/100], Step [100/135], Loss: 1.4279\n",
      "Epoch [58/100], Step [100/135], Loss: 1.4024\n",
      "Epoch [59/100], Step [100/135], Loss: 1.4011\n",
      "Epoch [60/100], Step [100/135], Loss: 1.4010\n",
      "Epoch [61/100], Step [100/135], Loss: 1.4009\n",
      "Epoch [62/100], Step [100/135], Loss: 1.4009\n",
      "Epoch [63/100], Step [100/135], Loss: 1.4008\n",
      "Epoch [64/100], Step [100/135], Loss: 1.4008\n",
      "Epoch [65/100], Step [100/135], Loss: 1.4007\n",
      "Epoch [66/100], Step [100/135], Loss: 1.4007\n",
      "Epoch [67/100], Step [100/135], Loss: 1.4307\n",
      "Epoch [68/100], Step [100/135], Loss: 1.4028\n",
      "Epoch [69/100], Step [100/135], Loss: 1.4309\n",
      "Epoch [70/100], Step [100/135], Loss: 1.4281\n",
      "Epoch [71/100], Step [100/135], Loss: 1.4022\n",
      "Epoch [72/100], Step [100/135], Loss: 1.4308\n",
      "Epoch [73/100], Step [100/135], Loss: 1.4280\n",
      "Epoch [74/100], Step [100/135], Loss: 1.4274\n",
      "Epoch [75/100], Step [100/135], Loss: 1.4020\n",
      "Epoch [76/100], Step [100/135], Loss: 1.4306\n",
      "Epoch [77/100], Step [100/135], Loss: 1.4026\n",
      "Epoch [78/100], Step [100/135], Loss: 1.4307\n",
      "Epoch [79/100], Step [100/135], Loss: 1.4278\n",
      "Epoch [80/100], Step [100/135], Loss: 1.4273\n",
      "Epoch [81/100], Step [100/135], Loss: 1.4271\n",
      "Epoch [82/100], Step [100/135], Loss: 1.4019\n",
      "Epoch [83/100], Step [100/135], Loss: 1.4304\n",
      "Epoch [84/100], Step [100/135], Loss: 1.4276\n",
      "Epoch [85/100], Step [100/135], Loss: 1.4020\n",
      "Epoch [86/100], Step [100/135], Loss: 1.4278\n",
      "Epoch [87/100], Step [100/135], Loss: 1.4271\n",
      "Epoch [88/100], Step [100/135], Loss: 1.4269\n",
      "Epoch [89/100], Step [100/135], Loss: 1.4019\n",
      "Epoch [90/100], Step [100/135], Loss: 1.4277\n",
      "Epoch [91/100], Step [100/135], Loss: 1.4270\n",
      "Epoch [92/100], Step [100/135], Loss: 1.4268\n",
      "Epoch [93/100], Step [100/135], Loss: 1.4018\n",
      "Epoch [94/100], Step [100/135], Loss: 1.4276\n",
      "Epoch [95/100], Step [100/135], Loss: 1.4268\n",
      "Epoch [96/100], Step [100/135], Loss: 1.4267\n",
      "Epoch [97/100], Step [100/135], Loss: 1.4018\n",
      "Epoch [98/100], Step [100/135], Loss: 1.4275\n",
      "Epoch [99/100], Step [100/135], Loss: 1.4267\n",
      "Epoch [100/100], Step [100/135], Loss: 1.4265\n",
      "Test set: Average loss: 0.1034, Accuracy: 6/15 (40.00%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [2/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [3/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [4/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [5/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [6/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [7/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [8/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [9/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [10/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [11/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [12/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [13/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [14/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [15/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [16/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [17/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [18/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [19/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [20/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [21/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [22/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [23/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [24/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [25/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [26/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [27/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [28/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [29/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [30/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [31/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [32/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [33/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [34/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [35/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [36/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [37/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [38/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [39/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [40/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [41/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [42/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [43/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [44/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [45/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [46/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [47/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [48/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [49/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [50/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [51/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [52/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [53/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [54/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [55/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [56/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [57/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [58/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [59/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [60/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [61/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [62/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [63/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [64/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [65/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [66/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [67/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [68/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [69/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [70/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [71/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [72/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [73/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [74/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [75/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [76/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [77/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [78/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [79/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [80/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [81/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [82/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [83/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [84/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [85/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [86/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [87/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [88/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [89/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [90/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [91/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [92/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [93/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [94/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [95/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [96/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [97/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [98/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [99/100], Step [100/135], Loss: 1.1798\n",
      "Epoch [100/100], Step [100/135], Loss: 1.1798\n",
      "Test set: Average loss: 0.0368, Accuracy: 7/15 (46.67%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [2/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [3/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [4/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [5/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [6/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [7/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [8/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [9/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [10/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [11/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [12/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [13/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [14/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [15/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [16/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [17/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [18/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [19/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [20/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [21/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [22/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [23/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [24/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [25/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [26/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [27/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [28/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [29/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [30/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [31/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [32/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [33/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [34/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [35/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [36/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [37/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [38/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [39/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [40/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [41/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [42/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [43/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [44/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [45/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [46/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [47/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [48/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [49/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [50/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [51/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [52/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [53/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [54/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [55/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [56/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [57/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [58/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [59/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [60/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [61/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [62/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [63/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [64/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [65/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [66/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [67/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [68/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [69/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [70/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [71/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [72/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [73/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [74/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [75/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [76/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [77/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [78/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [79/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [80/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [81/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [82/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [83/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [84/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [85/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [86/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [87/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [88/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [89/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [90/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [91/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [92/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [93/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [94/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [95/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [96/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [97/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [98/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [99/100], Step [100/135], Loss: 1.5514\n",
      "Epoch [100/100], Step [100/135], Loss: 1.5514\n",
      "Test set: Average loss: 0.1034, Accuracy: 3/15 (20.00%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 0.9656\n",
      "Epoch [2/100], Step [100/135], Loss: 0.7112\n",
      "Epoch [3/100], Step [100/135], Loss: 0.6979\n",
      "Epoch [4/100], Step [100/135], Loss: 0.6737\n",
      "Epoch [5/100], Step [100/135], Loss: 0.6704\n",
      "Epoch [6/100], Step [100/135], Loss: 0.6764\n",
      "Epoch [7/100], Step [100/135], Loss: 0.7120\n",
      "Epoch [8/100], Step [100/135], Loss: 0.6710\n",
      "Epoch [9/100], Step [100/135], Loss: 0.7102\n",
      "Epoch [10/100], Step [100/135], Loss: 0.6707\n",
      "Epoch [11/100], Step [100/135], Loss: 0.7099\n",
      "Epoch [12/100], Step [100/135], Loss: 0.6706\n",
      "Epoch [13/100], Step [100/135], Loss: 0.7099\n",
      "Epoch [14/100], Step [100/135], Loss: 0.6705\n",
      "Epoch [15/100], Step [100/135], Loss: 0.7100\n",
      "Epoch [16/100], Step [100/135], Loss: 0.6705\n",
      "Epoch [17/100], Step [100/135], Loss: 0.7101\n",
      "Epoch [18/100], Step [100/135], Loss: 0.6705\n",
      "Epoch [19/100], Step [100/135], Loss: 0.7102\n",
      "Epoch [20/100], Step [100/135], Loss: 0.6705\n",
      "Epoch [21/100], Step [100/135], Loss: 0.7104\n",
      "Epoch [22/100], Step [100/135], Loss: 0.6705\n",
      "Epoch [23/100], Step [100/135], Loss: 0.7106\n",
      "Epoch [24/100], Step [100/135], Loss: 0.6706\n",
      "Epoch [25/100], Step [100/135], Loss: 0.7108\n",
      "Epoch [26/100], Step [100/135], Loss: 0.6706\n",
      "Epoch [27/100], Step [100/135], Loss: 0.7110\n",
      "Epoch [28/100], Step [100/135], Loss: 0.7141\n",
      "Epoch [29/100], Step [100/135], Loss: 0.6709\n",
      "Epoch [30/100], Step [100/135], Loss: 0.7105\n",
      "Epoch [31/100], Step [100/135], Loss: 0.6707\n",
      "Epoch [32/100], Step [100/135], Loss: 0.7108\n",
      "Epoch [33/100], Step [100/135], Loss: 0.6707\n",
      "Epoch [34/100], Step [100/135], Loss: 0.7112\n",
      "Epoch [35/100], Step [100/135], Loss: 0.6707\n",
      "Epoch [36/100], Step [100/135], Loss: 0.7115\n",
      "Epoch [37/100], Step [100/135], Loss: 0.6707\n",
      "Epoch [38/100], Step [100/135], Loss: 0.7118\n",
      "Epoch [39/100], Step [100/135], Loss: 0.7152\n",
      "Epoch [40/100], Step [100/135], Loss: 0.6711\n",
      "Epoch [41/100], Step [100/135], Loss: 0.7113\n",
      "Epoch [42/100], Step [100/135], Loss: 0.6708\n",
      "Epoch [43/100], Step [100/135], Loss: 0.7117\n",
      "Epoch [44/100], Step [100/135], Loss: 0.6708\n",
      "Epoch [45/100], Step [100/135], Loss: 0.7121\n",
      "Epoch [46/100], Step [100/135], Loss: 0.6708\n",
      "Epoch [47/100], Step [100/135], Loss: 0.7124\n",
      "Epoch [48/100], Step [100/135], Loss: 0.6708\n",
      "Epoch [49/100], Step [100/135], Loss: 0.7127\n",
      "Epoch [50/100], Step [100/135], Loss: 0.7537\n",
      "Epoch [51/100], Step [100/135], Loss: 0.6729\n",
      "Epoch [52/100], Step [100/135], Loss: 0.7123\n",
      "Epoch [53/100], Step [100/135], Loss: 0.6710\n",
      "Epoch [54/100], Step [100/135], Loss: 0.7123\n",
      "Epoch [55/100], Step [100/135], Loss: 0.6709\n",
      "Epoch [56/100], Step [100/135], Loss: 0.7495\n",
      "Epoch [57/100], Step [100/135], Loss: 0.6728\n",
      "Epoch [58/100], Step [100/135], Loss: 0.7499\n",
      "Epoch [59/100], Step [100/135], Loss: 0.6728\n",
      "Epoch [60/100], Step [100/135], Loss: 0.7498\n",
      "Epoch [61/100], Step [100/135], Loss: 0.6728\n",
      "Epoch [62/100], Step [100/135], Loss: 0.7498\n",
      "Epoch [63/100], Step [100/135], Loss: 0.6728\n",
      "Epoch [64/100], Step [100/135], Loss: 0.7499\n",
      "Epoch [65/100], Step [100/135], Loss: 0.6728\n",
      "Epoch [66/100], Step [100/135], Loss: 0.7500\n",
      "Epoch [67/100], Step [100/135], Loss: 0.6728\n",
      "Epoch [68/100], Step [100/135], Loss: 0.7502\n",
      "Epoch [69/100], Step [100/135], Loss: 0.6727\n",
      "Epoch [70/100], Step [100/135], Loss: 0.7504\n",
      "Epoch [71/100], Step [100/135], Loss: 0.6727\n",
      "Epoch [72/100], Step [100/135], Loss: 0.7505\n",
      "Epoch [73/100], Step [100/135], Loss: 0.6921\n",
      "Epoch [74/100], Step [100/135], Loss: 0.6699\n",
      "Epoch [75/100], Step [100/135], Loss: 0.7513\n",
      "Epoch [76/100], Step [100/135], Loss: 0.6724\n",
      "Epoch [77/100], Step [100/135], Loss: 0.7519\n",
      "Epoch [78/100], Step [100/135], Loss: 0.6918\n",
      "Epoch [79/100], Step [100/135], Loss: 0.7559\n",
      "Epoch [80/100], Step [100/135], Loss: 0.6918\n",
      "Epoch [81/100], Step [100/135], Loss: 0.6695\n",
      "Epoch [82/100], Step [100/135], Loss: 0.7518\n",
      "Epoch [83/100], Step [100/135], Loss: 0.6914\n",
      "Epoch [84/100], Step [100/135], Loss: 0.7565\n",
      "Epoch [85/100], Step [100/135], Loss: 0.6915\n",
      "Epoch [86/100], Step [100/135], Loss: 0.6691\n",
      "Epoch [87/100], Step [100/135], Loss: 0.7523\n",
      "Epoch [88/100], Step [100/135], Loss: 0.6911\n",
      "Epoch [89/100], Step [100/135], Loss: 0.7571\n",
      "Epoch [90/100], Step [100/135], Loss: 0.6961\n",
      "Epoch [91/100], Step [100/135], Loss: 0.6694\n",
      "Epoch [92/100], Step [100/135], Loss: 0.7528\n",
      "Epoch [93/100], Step [100/135], Loss: 0.6963\n",
      "Epoch [94/100], Step [100/135], Loss: 0.6692\n",
      "Epoch [95/100], Step [100/135], Loss: 0.7538\n",
      "Epoch [96/100], Step [100/135], Loss: 0.6965\n",
      "Epoch [97/100], Step [100/135], Loss: 0.6690\n",
      "Epoch [98/100], Step [100/135], Loss: 0.7545\n",
      "Epoch [99/100], Step [100/135], Loss: 0.6968\n",
      "Epoch [100/100], Step [100/135], Loss: 0.6688\n",
      "Test set: Average loss: 0.0368, Accuracy: 9/15 (60.00%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 1.1440\n",
      "Epoch [2/100], Step [100/135], Loss: 1.4050\n",
      "Epoch [3/100], Step [100/135], Loss: 1.4126\n",
      "Epoch [4/100], Step [100/135], Loss: 1.4090\n",
      "Epoch [5/100], Step [100/135], Loss: 1.4091\n",
      "Epoch [6/100], Step [100/135], Loss: 1.4098\n",
      "Epoch [7/100], Step [100/135], Loss: 1.4104\n",
      "Epoch [8/100], Step [100/135], Loss: 1.4110\n",
      "Epoch [9/100], Step [100/135], Loss: 1.4113\n",
      "Epoch [10/100], Step [100/135], Loss: 1.4117\n",
      "Epoch [11/100], Step [100/135], Loss: 1.4119\n",
      "Epoch [12/100], Step [100/135], Loss: 1.4121\n",
      "Epoch [13/100], Step [100/135], Loss: 1.4123\n",
      "Epoch [14/100], Step [100/135], Loss: 1.4124\n",
      "Epoch [15/100], Step [100/135], Loss: 1.4125\n",
      "Epoch [16/100], Step [100/135], Loss: 1.4126\n",
      "Epoch [17/100], Step [100/135], Loss: 1.4127\n",
      "Epoch [18/100], Step [100/135], Loss: 1.4128\n",
      "Epoch [19/100], Step [100/135], Loss: 1.3267\n",
      "Epoch [20/100], Step [100/135], Loss: 1.3405\n",
      "Epoch [21/100], Step [100/135], Loss: 1.4098\n",
      "Epoch [22/100], Step [100/135], Loss: 1.4039\n",
      "Epoch [23/100], Step [100/135], Loss: 1.4124\n",
      "Epoch [24/100], Step [100/135], Loss: 1.4165\n",
      "Epoch [25/100], Step [100/135], Loss: 1.4188\n",
      "Epoch [26/100], Step [100/135], Loss: 1.4196\n",
      "Epoch [27/100], Step [100/135], Loss: 1.4200\n",
      "Epoch [28/100], Step [100/135], Loss: 1.4202\n",
      "Epoch [29/100], Step [100/135], Loss: 1.4204\n",
      "Epoch [30/100], Step [100/135], Loss: 1.4205\n",
      "Epoch [31/100], Step [100/135], Loss: 1.4206\n",
      "Epoch [32/100], Step [100/135], Loss: 1.4207\n",
      "Epoch [33/100], Step [100/135], Loss: 1.3228\n",
      "Epoch [34/100], Step [100/135], Loss: 1.3387\n",
      "Epoch [35/100], Step [100/135], Loss: 1.4102\n",
      "Epoch [36/100], Step [100/135], Loss: 1.3628\n",
      "Epoch [37/100], Step [100/135], Loss: 1.4127\n",
      "Epoch [38/100], Step [100/135], Loss: 1.3638\n",
      "Epoch [39/100], Step [100/135], Loss: 1.4126\n",
      "Epoch [40/100], Step [100/135], Loss: 1.3634\n",
      "Epoch [41/100], Step [100/135], Loss: 1.4123\n",
      "Epoch [42/100], Step [100/135], Loss: 1.3630\n",
      "Epoch [43/100], Step [100/135], Loss: 1.4121\n",
      "Epoch [44/100], Step [100/135], Loss: 1.3627\n",
      "Epoch [45/100], Step [100/135], Loss: 1.4118\n",
      "Epoch [46/100], Step [100/135], Loss: 1.3623\n",
      "Epoch [47/100], Step [100/135], Loss: 1.4116\n",
      "Epoch [48/100], Step [100/135], Loss: 1.3620\n",
      "Epoch [49/100], Step [100/135], Loss: 1.4240\n",
      "Epoch [50/100], Step [100/135], Loss: 1.3696\n",
      "Epoch [51/100], Step [100/135], Loss: 1.4128\n",
      "Epoch [52/100], Step [100/135], Loss: 1.3625\n",
      "Epoch [53/100], Step [100/135], Loss: 1.4243\n",
      "Epoch [54/100], Step [100/135], Loss: 1.3696\n",
      "Epoch [55/100], Step [100/135], Loss: 1.4129\n",
      "Epoch [56/100], Step [100/135], Loss: 1.3622\n",
      "Epoch [57/100], Step [100/135], Loss: 1.4244\n",
      "Epoch [58/100], Step [100/135], Loss: 1.3694\n",
      "Epoch [59/100], Step [100/135], Loss: 1.4128\n",
      "Epoch [60/100], Step [100/135], Loss: 1.3620\n",
      "Epoch [61/100], Step [100/135], Loss: 1.4245\n",
      "Epoch [62/100], Step [100/135], Loss: 1.3693\n",
      "Epoch [63/100], Step [100/135], Loss: 1.4127\n",
      "Epoch [64/100], Step [100/135], Loss: 1.3617\n",
      "Epoch [65/100], Step [100/135], Loss: 1.4175\n",
      "Epoch [66/100], Step [100/135], Loss: 1.3641\n",
      "Epoch [67/100], Step [100/135], Loss: 1.4178\n",
      "Epoch [68/100], Step [100/135], Loss: 1.3642\n",
      "Epoch [69/100], Step [100/135], Loss: 1.4178\n",
      "Epoch [70/100], Step [100/135], Loss: 1.3640\n",
      "Epoch [71/100], Step [100/135], Loss: 1.4178\n",
      "Epoch [72/100], Step [100/135], Loss: 1.3639\n",
      "Epoch [73/100], Step [100/135], Loss: 1.4177\n",
      "Epoch [74/100], Step [100/135], Loss: 1.3637\n",
      "Epoch [75/100], Step [100/135], Loss: 1.4176\n",
      "Epoch [76/100], Step [100/135], Loss: 1.3635\n",
      "Epoch [77/100], Step [100/135], Loss: 1.4176\n",
      "Epoch [78/100], Step [100/135], Loss: 1.3633\n",
      "Epoch [79/100], Step [100/135], Loss: 1.4175\n",
      "Epoch [80/100], Step [100/135], Loss: 1.3631\n",
      "Epoch [81/100], Step [100/135], Loss: 1.4174\n",
      "Epoch [82/100], Step [100/135], Loss: 1.3629\n",
      "Epoch [83/100], Step [100/135], Loss: 1.4173\n",
      "Epoch [84/100], Step [100/135], Loss: 1.3627\n",
      "Epoch [85/100], Step [100/135], Loss: 1.4173\n",
      "Epoch [86/100], Step [100/135], Loss: 1.3625\n",
      "Epoch [87/100], Step [100/135], Loss: 1.4172\n",
      "Epoch [88/100], Step [100/135], Loss: 1.3623\n",
      "Epoch [89/100], Step [100/135], Loss: 1.4171\n",
      "Epoch [90/100], Step [100/135], Loss: 1.3621\n",
      "Epoch [91/100], Step [100/135], Loss: 1.4170\n",
      "Epoch [92/100], Step [100/135], Loss: 1.3619\n",
      "Epoch [93/100], Step [100/135], Loss: 1.4169\n",
      "Epoch [94/100], Step [100/135], Loss: 1.3617\n",
      "Epoch [95/100], Step [100/135], Loss: 1.4168\n",
      "Epoch [96/100], Step [100/135], Loss: 1.3615\n",
      "Epoch [97/100], Step [100/135], Loss: 1.4167\n",
      "Epoch [98/100], Step [100/135], Loss: 1.3613\n",
      "Epoch [99/100], Step [100/135], Loss: 1.4166\n",
      "Epoch [100/100], Step [100/135], Loss: 1.3611\n",
      "Test set: Average loss: 0.1034, Accuracy: 5/15 (33.33%)\n",
      "\n",
      "Epoch [1/100], Step [100/135], Loss: 1.1132\n",
      "Epoch [2/100], Step [100/135], Loss: 1.3300\n",
      "Epoch [3/100], Step [100/135], Loss: 1.3800\n",
      "Epoch [4/100], Step [100/135], Loss: 1.4293\n",
      "Epoch [5/100], Step [100/135], Loss: 1.3459\n",
      "Epoch [6/100], Step [100/135], Loss: 1.4222\n",
      "Epoch [7/100], Step [100/135], Loss: 1.4017\n",
      "Epoch [8/100], Step [100/135], Loss: 1.4002\n",
      "Epoch [9/100], Step [100/135], Loss: 1.4000\n",
      "Epoch [10/100], Step [100/135], Loss: 1.4056\n",
      "Epoch [11/100], Step [100/135], Loss: 1.3949\n",
      "Epoch [12/100], Step [100/135], Loss: 1.4042\n",
      "Epoch [13/100], Step [100/135], Loss: 1.3945\n",
      "Epoch [14/100], Step [100/135], Loss: 1.3907\n",
      "Epoch [15/100], Step [100/135], Loss: 1.4029\n",
      "Epoch [16/100], Step [100/135], Loss: 1.3939\n",
      "Epoch [17/100], Step [100/135], Loss: 1.3903\n",
      "Epoch [18/100], Step [100/135], Loss: 1.3890\n",
      "Epoch [19/100], Step [100/135], Loss: 1.4021\n",
      "Epoch [20/100], Step [100/135], Loss: 1.3933\n",
      "Epoch [21/100], Step [100/135], Loss: 1.3898\n",
      "Epoch [22/100], Step [100/135], Loss: 1.3885\n",
      "Epoch [23/100], Step [100/135], Loss: 1.3879\n",
      "Epoch [24/100], Step [100/135], Loss: 1.4013\n",
      "Epoch [25/100], Step [100/135], Loss: 1.3925\n",
      "Epoch [26/100], Step [100/135], Loss: 1.3891\n",
      "Epoch [27/100], Step [100/135], Loss: 1.3877\n",
      "Epoch [28/100], Step [100/135], Loss: 1.3871\n",
      "Epoch [29/100], Step [100/135], Loss: 1.4005\n",
      "Epoch [30/100], Step [100/135], Loss: 1.3918\n",
      "Epoch [31/100], Step [100/135], Loss: 1.3883\n",
      "Epoch [32/100], Step [100/135], Loss: 1.3870\n",
      "Epoch [33/100], Step [100/135], Loss: 1.3864\n",
      "Epoch [34/100], Step [100/135], Loss: 1.3860\n",
      "Epoch [35/100], Step [100/135], Loss: 1.3995\n",
      "Epoch [36/100], Step [100/135], Loss: 1.3908\n",
      "Epoch [37/100], Step [100/135], Loss: 1.3873\n",
      "Epoch [38/100], Step [100/135], Loss: 1.3860\n",
      "Epoch [39/100], Step [100/135], Loss: 1.3854\n",
      "Epoch [40/100], Step [100/135], Loss: 1.3851\n",
      "Epoch [41/100], Step [100/135], Loss: 1.3848\n",
      "Epoch [42/100], Step [100/135], Loss: 1.3846\n",
      "Epoch [43/100], Step [100/135], Loss: 1.3981\n",
      "Epoch [44/100], Step [100/135], Loss: 1.3894\n",
      "Epoch [45/100], Step [100/135], Loss: 1.3859\n",
      "Epoch [46/100], Step [100/135], Loss: 1.3846\n",
      "Epoch [47/100], Step [100/135], Loss: 1.3840\n",
      "Epoch [48/100], Step [100/135], Loss: 1.3837\n",
      "Epoch [49/100], Step [100/135], Loss: 1.3834\n",
      "Epoch [50/100], Step [100/135], Loss: 1.3832\n",
      "Epoch [51/100], Step [100/135], Loss: 1.3830\n",
      "Epoch [52/100], Step [100/135], Loss: 1.3966\n",
      "Epoch [53/100], Step [100/135], Loss: 1.3878\n",
      "Epoch [54/100], Step [100/135], Loss: 1.3844\n",
      "Epoch [55/100], Step [100/135], Loss: 1.3830\n",
      "Epoch [56/100], Step [100/135], Loss: 1.3825\n",
      "Epoch [57/100], Step [100/135], Loss: 1.3821\n",
      "Epoch [58/100], Step [100/135], Loss: 1.3819\n",
      "Epoch [59/100], Step [100/135], Loss: 1.3817\n",
      "Epoch [60/100], Step [100/135], Loss: 1.3814\n",
      "Epoch [61/100], Step [100/135], Loss: 1.3812\n",
      "Epoch [62/100], Step [100/135], Loss: 1.3810\n",
      "Epoch [63/100], Step [100/135], Loss: 1.3947\n",
      "Epoch [64/100], Step [100/135], Loss: 1.3858\n",
      "Epoch [65/100], Step [100/135], Loss: 1.3824\n",
      "Epoch [66/100], Step [100/135], Loss: 1.3811\n",
      "Epoch [67/100], Step [100/135], Loss: 1.3805\n",
      "Epoch [68/100], Step [100/135], Loss: 1.3802\n",
      "Epoch [69/100], Step [100/135], Loss: 1.3799\n",
      "Epoch [70/100], Step [100/135], Loss: 1.3797\n",
      "Epoch [71/100], Step [100/135], Loss: 1.3795\n",
      "Epoch [72/100], Step [100/135], Loss: 1.3793\n",
      "Epoch [73/100], Step [100/135], Loss: 1.3791\n",
      "Epoch [74/100], Step [100/135], Loss: 1.3789\n",
      "Epoch [75/100], Step [100/135], Loss: 1.3926\n",
      "Epoch [76/100], Step [100/135], Loss: 1.3837\n",
      "Epoch [77/100], Step [100/135], Loss: 1.3802\n",
      "Epoch [78/100], Step [100/135], Loss: 1.3789\n",
      "Epoch [79/100], Step [100/135], Loss: 1.3783\n",
      "Epoch [80/100], Step [100/135], Loss: 1.3780\n",
      "Epoch [81/100], Step [100/135], Loss: 1.3777\n",
      "Epoch [82/100], Step [100/135], Loss: 1.3775\n",
      "Epoch [83/100], Step [100/135], Loss: 1.3773\n",
      "Epoch [84/100], Step [100/135], Loss: 1.3771\n",
      "Epoch [85/100], Step [100/135], Loss: 1.3769\n",
      "Epoch [86/100], Step [100/135], Loss: 1.3767\n",
      "Epoch [87/100], Step [100/135], Loss: 1.3765\n",
      "Epoch [88/100], Step [100/135], Loss: 1.3903\n",
      "Epoch [89/100], Step [100/135], Loss: 1.3812\n",
      "Epoch [90/100], Step [100/135], Loss: 1.3778\n",
      "Epoch [91/100], Step [100/135], Loss: 1.3765\n",
      "Epoch [92/100], Step [100/135], Loss: 1.3759\n",
      "Epoch [93/100], Step [100/135], Loss: 1.3755\n",
      "Epoch [94/100], Step [100/135], Loss: 1.3753\n",
      "Epoch [95/100], Step [100/135], Loss: 1.3751\n",
      "Epoch [96/100], Step [100/135], Loss: 1.3748\n",
      "Epoch [97/100], Step [100/135], Loss: 1.3746\n",
      "Epoch [98/100], Step [100/135], Loss: 1.3744\n",
      "Epoch [99/100], Step [100/135], Loss: 1.3883\n",
      "Epoch [100/100], Step [100/135], Loss: 1.3792\n",
      "Test set: Average loss: 0.1034, Accuracy: 6/15 (40.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sampo.scheduler.selection.neural_net import one_hot_encode, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "s = cross_val_score(train_dataset=train_dataset,\n",
    "                      target_column='label',\n",
    "                      model=model,\n",
    "                      epochs=100,\n",
    "                      folds=10,\n",
    "                      shuffle=True,\n",
    "                      random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T16:33:50.888163Z",
     "start_time": "2023-08-28T16:32:34.871009100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.333333333333336, 40.0, 33.333333333333336, 40.0, 40.0, 46.666666666666664, 20.0, 60.0, 33.333333333333336, 40.0]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T16:34:05.388475Z",
     "start_time": "2023-08-28T16:34:05.372698900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
